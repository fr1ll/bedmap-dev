{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import shutil\n",
    "from dataclasses import dataclass, field\n",
    "from glob import glob\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "\n",
    "import memray\n",
    "import daft\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext memray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DAFT: bool = False # else use torch dataset\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MODEL_NAME = \"vit_base_patch14_reg4_dinov2.lvd142m\"\n",
    "IMAGE_GLOB = None\n",
    "IMAGES_FOLDER = \"./tmp-test-images\"\n",
    "\n",
    "TEST_DATASET = \"kvriza8/microscopy_images\"\n",
    "NUM_TEST_IMAGES = 500\n",
    "\n",
    "nice_models = [\n",
    "\"mobilenetv3_large_100\",\n",
    "\"vit_small_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_large_patch14_reg4_dinov2.lvd142m\",\n",
    "\"aimv2_large_patch14_224.apple_pt_dist\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_hf_images(dataset_name: str = \"kvriza8/microscopy_images\",\n",
    "                 dir: Path = None,\n",
    "                 max_images: int = 50,\n",
    "                 overwrite: bool = True) -> None:\n",
    "\n",
    "    dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "    if overwrite:\n",
    "        shutil.rmtree(dir, ignore_errors=True)\n",
    "        dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i, img_row in enumerate(tqdm(iter(dataset), total=max_images)):\n",
    "        if i >= max_images:\n",
    "            break\n",
    "        img = img_row[\"image\"]\n",
    "        img.save(dir / f\"{i}.png\")\n",
    "\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:13<00:00, 36.39it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "tmp_path = Path(IMAGES_FOLDER)\n",
    "dl_hf_images(dir=tmp_path, max_images=NUM_TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Embedder:\n",
    "    model_name: str\n",
    "    device: torch.device = field(default_factory=lambda: torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    dtype: torch.dtype = field(init=False)\n",
    "    model: torch.nn.Module = field(init=False)\n",
    "    transform: callable = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.dtype = torch.bfloat16 if self.device.type == \"cuda\" and torch.cuda.is_bf16_supported() else (\n",
    "            torch.float16 if self.device.type == \"cuda\" else torch.float32\n",
    "        )\n",
    "        self.model = timm.create_model(self.model_name, pretrained=True, num_classes=0)\n",
    "        self.model.to(self.device, memory_format=torch.channels_last)\n",
    "        self.model.eval()\n",
    "        self.model = torch.compile(self.model, dynamic=True, mode=\"reduce-overhead\")\n",
    "        # Resolve config removes unneeded fields before create_transform\n",
    "        cfg = timm.data.resolve_data_config(self.model.pretrained_cfg)\n",
    "        self.transform = timm.data.create_transform(**cfg)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def embed(self, batch_imgs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Given a batch of pre-transformed images, compute pooled embeddings.\n",
    "        The batch is moved to the proper device (with channels_last format) and processed in inference mode.\n",
    "        \"\"\"\n",
    "        batch_imgs.to(self.device, non_blocking=True, memory_format=torch.channels_last)\n",
    "        if self.device.type == \"cuda\":\n",
    "            with torch.amp.autocast(\"cuda\", dtype=self.dtype):\n",
    "                return self.model(batch_imgs)\n",
    "        else:\n",
    "            # autocast can be comically slow for some CPU setups (PyTorch issue #118499)\n",
    "            return self.model(batch_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@daft.udf(return_dtype=daft.DataType.python())\n",
    "class TransformImageCol:\n",
    "    \"\"\"run timm embedder on an image column\"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.embedder = Embedder(self.model_name)\n",
    "\n",
    "    def __call__(self, batch_images) -> list:\n",
    "        return [self.embedder.transform(Image.fromarray(im)) for im in batch_images.to_pylist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageListIteratorAsDict(Dataset):\n",
    "    def __init__(self, filelist: list[Path], transform: callable):\n",
    "        self.filelist = filelist\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filelist)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image = Image.open(self.filelist[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            return {\"image_transformed\": self.transform(image)}\n",
    "        # return as dict for easy comparison vs. daft\n",
    "        else:\n",
    "            return {\"image\": [image]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_file_list(source: str | list[str]) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Given a folder path, a glob pattern, or a filelist, return a list of image file paths.\n",
    "    If source is a directory, a glob is run using the provided pattern.\n",
    "    If source is a string containing a wildcard, glob is applied.\n",
    "    Otherwise, if it's a list, it is returned directly.\n",
    "    \"\"\"\n",
    "    if isinstance(source, list):\n",
    "        return [Path(s) for s in source]\n",
    "    elif Path(source).is_dir():\n",
    "        patterns = [\"*.png\", \"*.jpg\", \"*.jpeg\"]\n",
    "        return list(chain.from_iterable([Path(source).glob(p) for p in patterns]))\n",
    "    elif isinstance(source, str) and '*' in source:\n",
    "        return [Path(p) for p in glob(source)]\n",
    "    else:\n",
    "        return [source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table class=\"dataframe\">\n",
       "<thead><tr><th style=\"text-wrap: nowrap; max-width:192px; overflow:auto; text-align:left\">path_full_img<br />Utf8</th><th style=\"text-wrap: nowrap; max-width:192px; overflow:auto; text-align:left\">size<br />Int64</th><th style=\"text-wrap: nowrap; max-width:192px; overflow:auto; text-align:left\">image_transformed<br />Python</th></tr></thead>\n",
       "<tbody>\n",
       "<tr><td><div style=\"text-align:left; max-width:192px; max-height:64px; overflow:auto\">file://tmp-test-images/60.png</div></td><td><div style=\"text-align:left; max-width:192px; max-height:64px; overflow:auto\">37758</div></td><td><div style=\"text-align:left; max-width:192px; max-height:64px; overflow:auto\">tensor([[[-0.7993, -0.7308, -0.6794,  ...,  0.6734,  1.5810,  2.2489],<br />         [-0.9363, -0.8678, -0.8164,  ...,  0.6049,  1.5468,  2.2489],<br />         [-1.2617, -1.2103, -1.1589,  ...,  0.4851,  1.4783,  2.2318],<br />         ...,<br />         [-0.4739, -0.5596, -0.6623,  ...,  0.3309,  1.4269,  2.2489],<br />         [-0.7822, -0.9363, -1.0219,  ...,  0.3309,  1.4269,  2.2489],<br />         [-0.9363, -1.1075, -1.2103,  ...,  0.3309,  1.4269,  2.2489]],<br /><br />        [[-0.6877, -0.6176, -0.5651,  ...,  0.8179,  1.7458,  2.4286],<br />         [-0.8277, -0.7577, -0.7052,  ...,  0.7479,  1.7108,  2.4286],<br />         [-1.1604, -1.1078, -1.0553,  ...,  0.6254,  1.6408,  2.4111],<br />         ...,<br />         [-0.3550, -0.4426, -0.5476,  ...,  0.4678,  1.5882,  2.4286],<br />         [-0.6702, -0.8277, -0.9153,  ...,  0.4678,  1.5882,  2.4286],<br />         [-0.8277, -1.0028, -1.1078,  ...,  0.4678,  1.5882,  2.4286]],<br /><br />        [[-0.4624, -0.3927, -0.3404,  ...,  1.0365,  1.9603,  2.6400],<br />         [-0.6018, -0.5321, -0.4798,  ...,  0.9668,  1.9254,  2.6400],<br />         [-0.9330, -0.8807, -0.8284,  ...,  0.8448,  1.8557,  2.6226],<br />         ...,<br />         [-0.1312, -0.2184, -0.3230,  ...,  0.6879,  1.8034,  2.6400],<br />         [-0.4450, -0.6018, -0.6890,  ...,  0.6879,  1.8034,  2.6400],<br />         [-0.6018, -0.7761, -0.8807,  ...,  0.6879,  1.8034,  2.6400]]])</div></td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<small>(Showing first 1 rows)</small>\n",
       "</div>"
      ],
      "text/plain": [
       "╭───────────────────────────────┬───────┬────────────────────────────────╮\n",
       "│ path_full_img                 ┆ size  ┆ image_transformed              │\n",
       "│ ---                           ┆ ---   ┆ ---                            │\n",
       "│ Utf8                          ┆ Int64 ┆ Python                         │\n",
       "╞═══════════════════════════════╪═══════╪════════════════════════════════╡\n",
       "│ file://tmp-test-images/60.png ┆ 37758 ┆ tensor([[[-0.7993, -0.7308, -… │\n",
       "╰───────────────────────────────┴───────┴────────────────────────────────╯\n",
       "\n",
       "(Showing first 1 rows)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imglob = tmp_path.as_posix() +\"/*.png\"\n",
    "images_df = daft.from_glob_path(imglob).with_column_renamed(\"path\", \"path_full_img\")\n",
    "images_df = images_df.with_column(\"image\", daft.col(\"path_full_img\"\n",
    "                                 ).url.download().image.decode(\n",
    "                                     mode=\"RGB\", on_error=\"null\")\n",
    "                                 )\n",
    "images_df = images_df.where(images_df[\"image\"].not_null())\n",
    "\n",
    "TransformImForModel = TransformImageCol.with_init_args(model_name=MODEL_NAME)\n",
    "\n",
    "images_df = images_df.with_column(\"image_transformed\", TransformImForModel(daft.col(\"image\"))\n",
    "                                  ).exclude(\"image\", \"num_rows\")\n",
    "\n",
    "images_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(model_name: str,\n",
    "                       batch_size: int = BATCH_SIZE,\n",
    "                       images_df: None | daft.DataFrame = None,\n",
    "                       images_folder: None | Path = None) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Given a model name and a filelist (list of image paths), this function computes and returns a list\n",
    "    of embeddings (one per image). The function instantiates an Embedder, builds a dataset and dataloader,\n",
    "    and processes images in batches.\n",
    "    \"\"\"\n",
    "    embedder = Embedder(model_name=model_name)\n",
    "\n",
    "    if images_df is not None:\n",
    "        dataset = images_df.to_torch_iter_dataset()\n",
    "    elif images_folder:\n",
    "        # use vanilla torch dataloader\n",
    "        image_list = get_file_list(images_folder)\n",
    "        dataset = ImageListIteratorAsDict(image_list, embedder.transform)\n",
    "\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    embeddings = []\n",
    "    for i, batch_images in enumerate(tqdm(dataloader, unit_scale=BATCH_SIZE)):\n",
    "        emb = embedder.embed(batch_images[\"image_transformed\"]).cpu().numpy()\n",
    "\n",
    "        if i == 0:\n",
    "            print(f\"Shape of embedding for one batch: {emb.shape}\")\n",
    "        embeddings.append(emb)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memray WARNING: Correcting symbol for malloc from 0x420620 to 0x7f18cd97cc60\n",
      "Memray WARNING: Correcting symbol for free from 0x420ab0 to 0x7f18cd97d370\n",
      "  6%|▋         | 32/512 [03:51<57:49,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding for one batch: (32, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [12:07<00:00,  1.42s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8633d15e2954b65a635fb97ea1e7a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">⚠ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> No debug information was found for the Python interpreter </span> ⚠\n",
       "\n",
       "Without debug information reports showing native traces <span style=\"font-weight: bold\">may not include file names and line numbers</span>. Please use an \n",
       "interpreter built with debug symbols for best results. Check <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://bloomberg.github.io/memray/native_mode.html</span> \n",
       "for more information regarding how memray resolves symbols.\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "⚠ \u001b[1;33m No debug information was found for the Python interpreter \u001b[0m ⚠\n",
       "\n",
       "Without debug information reports showing native traces \u001b[1mmay not include file names and line numbers\u001b[0m. Please use an \n",
       "interpreter built with debug symbols for best results. Check \u001b[4;94mhttps://bloomberg.github.io/memray/native_mode.html\u001b[0m \n",
       "for more information regarding how memray resolves symbols.\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2befd14669384de38ccce369f395cee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Results saved to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">/home/willsa/git/bedmap-dev/nbs/daft-try/memray-results/tmpe57lnmpd/flamegraph.html</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Results saved to \u001b[1;36m/home/willsa/git/bedmap-dev/nbs/daft-try/memray-results/tmpe57lnmpd/\u001b[0m\u001b[1;36mflamegraph.html\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600\"\n",
       "            src=\"/home/willsa/git/bedmap-dev/nbs/daft-try/memray-results/tmpe57lnmpd/flamegraph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f14ef3a7e60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%memray_flamegraph --trace-python-allocators --native --leaks --temporal --max-memory-records 10\n",
    "\n",
    "if USE_DAFT:\n",
    "    embeddings = compute_embeddings(MODEL_NAME, BATCH_SIZE, images_df=images_df)\n",
    "else:\n",
    "    # use vanilla torch dataset\n",
    "    embeddings = compute_embeddings(MODEL_NAME, BATCH_SIZE, images_folder=IMAGES_FOLDER)\n",
    "\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
