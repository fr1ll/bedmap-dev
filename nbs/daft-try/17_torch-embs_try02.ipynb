{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 8\n",
    "MODEL_NAME = \"vit_large_patch14_reg4_dinov2.lvd142m\"\n",
    "IMAGE_GLOB = None\n",
    "IMAGES_FOLDER = \"./tmp-test-images\"\n",
    "\n",
    "TEST_DATASET = \"kvriza8/microscopy_images\"\n",
    "NUM_TEST_IMAGES = 2_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with vit_base_patch14:\n",
    "\n",
    "num_images | batch_size | optimize | time |\n",
    "-----------|------------|----------|------|\n",
    "500        |         32 | False    | 10:07 \n",
    "200        |         32 | False    | 04:50 \n",
    "2000       |         32 | False    | 41:00 \n",
    "50         |         32 | Static   | 01:14\n",
    "50         |         32 | Dynamic  | 01:14\n",
    "500        |         32 | Static   | 09:33 \n",
    "2000       | 32 (fixed) | Static   | 36:22\n",
    "2000       | 16 (fixed) | Static   | 36:32\n",
    "2000       |  4 (fixed) | Static   | 39:17\n",
    "2000       | 128 (fixd) | Static   | OOM\n",
    "2000lrg    |  16 (fixd) | Static   | -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_hf_images(dataset_name: str = \"kvriza8/microscopy_images\",\n",
    "                 dir: Path = None, max_images: int = 50) -> None:\n",
    "    dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "\n",
    "    for i, img_row in enumerate(tqdm(iter(dataset), total=max_images)):\n",
    "        if i >= max_images:\n",
    "            break\n",
    "        img = img_row[\"image\"]\n",
    "        img.save(dir / f\"{i}.png\")\n",
    "\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:35<00:00, 56.95it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "tmp_path = Path(IMAGES_FOLDER)\n",
    "shutil.rmtree(tmp_path, ignore_errors=True)\n",
    "tmp_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dl_hf_images(dir=tmp_path, max_images=NUM_TEST_IMAGES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_file_list(source: str | list[str], pattern: str = IMAGE_GLOB) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Given a folder path, a glob pattern, or a filelist, return a list of image file paths.\n",
    "    If source is a directory, a glob is run using the provided pattern.\n",
    "    If source is a string containing a wildcard, glob is applied.\n",
    "    Otherwise, if it's a list, it is returned directly.\n",
    "    \"\"\"\n",
    "    if isinstance(source, list):\n",
    "        return [Path(s) for s in source]\n",
    "    elif Path(source).is_dir():\n",
    "        patterns = [\"*.png\", \"*.jpg\", \"*.jpeg\"]\n",
    "        return list(chain.from_iterable([Path(source).glob(p) for p in patterns]))\n",
    "    elif isinstance(source, str) and '*' in source:\n",
    "        return [Path(p) for p in glob.glob(source)]\n",
    "    else:\n",
    "        return [source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Embedder:\n",
    "    model_name: str\n",
    "    device: torch.device = field(default_factory=lambda: torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model: torch.nn.Module = field(init=False)\n",
    "    transform: callable = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.model = timm.create_model(self.model_name, pretrained=True, num_classes=0)\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device, memory_format=torch.channels_last)\n",
    "        self.model = torch.compile(self.model, dynamic=True)\n",
    "        # Resolve config removes unneeded fields before create_transform\n",
    "        cfg = timm.data.resolve_data_config(self.model.pretrained_cfg, model=self.model)\n",
    "        self.transform = timm.data.create_transform(**cfg)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def embed(self, batch_imgs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Given a batch of pre-transformed images, compute pooled embeddings.\n",
    "        The batch is moved to the proper device (with channels_last format) and processed in inference mode.\n",
    "        \"\"\"\n",
    "        batch_imgs = batch_imgs.to(self.device, non_blocking=True)\n",
    "        batch_imgs = batch_imgs.contiguous(memory_format=torch.channels_last)\n",
    "        if self.device.type == \"cuda\":\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                return self.model(batch_imgs)\n",
    "        else:\n",
    "            # autocast can be comically slow for some CPU setups (PyTorch issue #118499)\n",
    "            return self.model(batch_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageListIterator(Dataset):\n",
    "    def __init__(self, filelist: list[Path], transform: callable):\n",
    "        self.filelist = filelist\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filelist)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image = Image.open(self.filelist[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_to_batch_size(batch, batch_size):\n",
    "    \"\"\"Pads the batch to batch_size with zeros\"\"\"\n",
    "    orig_size = len(batch)\n",
    "    batch = torch.stack(batch)\n",
    "    if orig_size < batch_size:\n",
    "        pad_tensor = torch.zeros((batch_size - orig_size, *batch.shape[1:]),\n",
    "                                 dtype=batch.dtype, device=batch.device)\n",
    "        batch = torch.cat([batch, pad_tensor], dim=0)\n",
    "    return batch, orig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_embeddings(model_name: str, filelist: list[str], batch_size: int = BATCH_SIZE) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Given a model name and a filelist (list of image paths), this function computes and returns a list\n",
    "    of embeddings (one per image). The function instantiates an Embedder, builds a dataset and dataloader,\n",
    "    and processes images in batches.\n",
    "    \"\"\"\n",
    "    embedder = Embedder(model_name=model_name)\n",
    "\n",
    "    dataset = ImageListIterator(filelist, embedder.transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=lambda b: pad_to_batch_size(b, batch_size),\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    for i, (batch_imgs, actual_batch_size) in enumerate(tqdm(dataloader)):\n",
    "        emb = embedder.embed(batch_imgs).cpu().numpy()\n",
    "        emb = emb[:actual_batch_size, ...]\n",
    "\n",
    "        if i == 0:\n",
    "            embeddings = emb\n",
    "            print(f\"Shape of embedding for one batch: {emb.shape}\")\n",
    "        else:\n",
    "            embeddings = np.concatenate((embeddings, emb), axis=0)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/125 [01:28<3:02:04, 88.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding for one batch: (16, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 109/125 [1:46:58<15:59, 59.96s/it] "
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "file_list = get_file_list(IMAGES_FOLDER)\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], profile_memory=True, record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "\n",
    "        embeddings = compute_embeddings(MODEL_NAME, file_list, BATCH_SIZE)\n",
    "\n",
    "print(f\"Processed {len(file_list)} images. Got {len(embeddings)} embeddings.\")\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: single '}' is not allowed (965503420.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    prof.export_chrome_trace(f\"trace_{MODEL_NAME}_{NUM_TEST_IMAGES}}x{BATCH_SIZE}.json\")\u001b[0m\n\u001b[0m                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: single '}' is not allowed\n"
     ]
    }
   ],
   "source": [
    "prof.export_chrome_trace(f\"trace_{MODEL_NAME}_{NUM_TEST_IMAGES}x{BATCH_SIZE}.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
