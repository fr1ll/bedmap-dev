{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp daft_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import daft\n",
    "import timm\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "daft.set_execution_config(enable_native_executor=True,\n",
    "                        #   default_morsel_size=50\n",
    "                          )\n",
    "\n",
    "MAX_IMAGES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:04, 11.35it/s]\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "tmp_path = Path(\"./tmp-test-images\")\n",
    "tmp_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dataset_name = \"kvriza8/microscopy_images\"\n",
    "\n",
    "dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "\n",
    "for i, example in enumerate(tqdm(iter(dataset))):  # Use iterator to avoid full load\n",
    "    if i >= MAX_IMAGES:\n",
    "        break\n",
    "    image = example[\"image\"]\n",
    "    image.save(tmp_path / f\"{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dataclass\n",
    "class TimmEmbedder:\n",
    "    \"\"\"\n",
    "    embed an image with any timm model that supports this\n",
    "\n",
    "    Reference: https://huggingface.co/docs/timm/main/en/feature_extraction#pooled\n",
    "    \"\"\"\n",
    "    model_name: str\n",
    "    device: torch.device = field(init=False)\n",
    "    dtype: torch.dtype = field(init=False)\n",
    "    _model: Callable = field(init=False)\n",
    "    _transforms: Callable = field(init=False)\n",
    "    _instance: \"TimmEmbedder\" = field(default=None, init=False, repr=False)\n",
    "\n",
    "    def __new__(cls, model_name):\n",
    "        \"\"\"make a singleton\"\"\"\n",
    "        if not hasattr(cls, \"_instance\") or cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if hasattr(self, \"_model\"):  # Avoid re-initialization\n",
    "            return\n",
    "        # initialize model and transforms\n",
    "        self._model = timm.create_model(self.model_name, pretrained=True, num_classes=0)\n",
    "        cfg = self._model.pretrained_cfg\n",
    "        self._transform = timm.data.create_transform(**timm.data.resolve_data_config(cfg))\n",
    "\n",
    "        # set device and dtype\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.dtype = torch.bfloat16 if self.device.type == \"cuda\" and torch.cuda.is_bf16_supported() else (\n",
    "            torch.float16 if self.device.type == \"cuda\" else torch.float32\n",
    "        )\n",
    "        print(f\"Inference device: {self.device} with dtype: {self.dtype}\")\n",
    "\n",
    "        # optimize model for inference\n",
    "        self._model = self._model.eval().to(device=self.device, dtype=self.dtype)\n",
    "        self._model = torch.compile(self._model)\n",
    "\n",
    "\n",
    "    def __call__(self, image: torch.Tensor | Image.Image) -> np.array:\n",
    "            \"\"\"transform image, run inference, extract embedding as 1D array\"\"\"\n",
    "            image = self._transform(image).to(self.device, self.dtype).unsqueeze(0)\n",
    "            emb = self._model(image)\n",
    "            return emb.detach().cpu().float().numpy().squeeze()\n",
    "\n",
    "    def transform(self, image: torch.Tensor | Image.Image) -> torch.Tensor:\n",
    "         \"\"\"transform image\"\"\"\n",
    "         return self._transform(image).to(self.device, self.dtype).unsqueeze(0)\n",
    "\n",
    "    def embed(self, img_tensor: torch.Tensor) -> np.array:\n",
    "         \"\"\"embed tensor of already-transformed images\"\"\"\n",
    "         return self._model(img_tensor).detach().cpu().float().numpy().squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference device: cpu with dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "# it's a one-stop shop\n",
    "e = TimmEmbedder(\"mobilenetv3_large_100\")\n",
    "e(torch.rand((3,256,256)))\n",
    "\n",
    "# or do in two steps\n",
    "\n",
    "t = e.transform(torch.rand((3,256,256)))\n",
    "v = e.embed(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@daft.udf(return_dtype=daft.DataType.list(daft.DataType.float32()),\n",
    "          memory_bytes=int(6e9))\n",
    "class EmbedImages:\n",
    "    \"\"\"run timm embedder on an image column\"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.embedder = TimmEmbedder(self.model_name)\n",
    "        \n",
    "    def __call__(self, images_col) -> list:\n",
    "        images = [rearrange(im, \"h w c -> c h w\") for im in images_col.to_pylist()]\n",
    "        images = torch.stack([self.embedder.transform(\n",
    "                    torch.tensor(im, dtype=self.embedder.dtype,\n",
    "                        device=self.embedder.device)) for im in images])\n",
    "        ## Example: https://colab.research.google.com/github/Eventual-Inc/Daft/blob/main/tutorials/mnist.ipynb\n",
    "        return self.embedder.embed(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: I need to figure out how to create two udfs that work together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "glob = tmp_path.as_posix() +\"/*.png\"\n",
    "images_df = daft.from_glob_path(glob).with_column_renamed(\"path\", \"path_full_img\")\n",
    "\n",
    "images_df = images_df.with_column(\"image\", daft.col(\"path_full_img\"\n",
    "                                 ).url.download().image.decode(\n",
    "                                     mode=\"RGB\", on_error=\"null\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference time for some embedding models on CPU on my laptop\n",
    "\n",
    "model_name | 50 images | 500 images | 1000 images | 2000 images\n",
    "---------- | --------- | ---------- | ----------- | -----------\n",
    "mobilenetv3_large_100 | 0m02s |  0m8s | 0m12s | 0m25s\n",
    "vit_base_patch14_reg4_dinov2.lvd142m | 1m20s | 11m48s | 23m29s | OOM\n",
    "vit_large_patch14_reg4_dinov2.lvd142m | 4m04s | 38m30s | OOM | OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "EmbedImagesWithModel = EmbedImages.with_init_args(\"mobilenetv3_large_100\")\n",
    "\n",
    "images_df = images_df.with_column(\"embed\", EmbedImagesWithModel(daft.col(\"image\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "images_df = images_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import shutil\n",
    "shutil.rmtree(tmp_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
