{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp daft_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import daft\n",
    "import timm\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from functools import partial\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "MAX_IMAGES = 500\n",
    "BATCH_SIZE = 24\n",
    "MODEL_NAME = \"vit_large_patch14_reg4_dinov2.lvd142m\"\n",
    "MEMORY_BYTES = int(6e9) # 6 GB memory allocation\n",
    "\n",
    "daft.set_execution_config(enable_native_executor=True,\n",
    "                          default_morsel_size=BATCH_SIZE\n",
    "                          )\n",
    "\n",
    "nice_models = [\n",
    "\"mobilenetv3_large_100\",\n",
    "\"vit_small_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_large_patch14_reg4_dinov2.lvd142m\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\n",
    "\"mobilenetv3_large_100\",\n",
    "\"vit_small_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_large_patch14_reg4_dinov2.lvd142m\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:11, 41.93it/s] \n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "tmp_path = Path(\"./tmp-test-images\")\n",
    "shutil.rmtree(tmp_path, ignore_errors=True)\n",
    "tmp_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dataset_name = \"kvriza8/microscopy_images\"\n",
    "\n",
    "dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "\n",
    "for i, example in enumerate(tqdm(iter(dataset))):  # Use iterator to avoid full load\n",
    "    if i >= MAX_IMAGES:\n",
    "        break\n",
    "    image = example[\"image\"]\n",
    "    image.save(tmp_path / f\"{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dataclass\n",
    "class TimmEmbedder:\n",
    "    \"\"\"\n",
    "    embed an image with any timm model that supports this\n",
    "\n",
    "    Reference: https://huggingface.co/docs/timm/main/en/feature_extraction#pooled\n",
    "    \"\"\"\n",
    "    model_name: str\n",
    "    device: torch.device = field(init=False)\n",
    "    dtype: torch.dtype = field(init=False)\n",
    "    _model: Callable = field(init=False)\n",
    "    _transforms: Callable = field(init=False)\n",
    "    _amp_autocast: Callable = field(default=None, init=False, repr=False)\n",
    "    _instance: \"TimmEmbedder\" = field(default=None, init=False, repr=False)\n",
    "\n",
    "    def __new__(cls, model_name):\n",
    "        \"\"\"make a singleton\"\"\"\n",
    "        if not hasattr(cls, \"_instance\") or cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if hasattr(self, \"_model\"):  # Avoid re-initialization\n",
    "            return\n",
    "        # initialize model and transforms\n",
    "        self._model = timm.create_model(self.model_name, pretrained=True, num_classes=0)\n",
    "        cfg = self._model.pretrained_cfg\n",
    "        self._transform = timm.data.create_transform(**timm.data.resolve_data_config(cfg))\n",
    "\n",
    "        # set device and dtype\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.dtype = torch.bfloat16 if self.device.type == \"cuda\" and torch.cuda.is_bf16_supported() else (\n",
    "            torch.float16 if self.device.type == \"cuda\" else torch.float32\n",
    "        )\n",
    "        _amp_dtype = torch.bfloat16 if self.dtype == torch.bfloat16 else torch.float16\n",
    "        self._amp_autocast = partial(torch.autocast, device_type=self.device.type, dtype=_amp_dtype)\n",
    "        print(f\"Inference device: {self.device} with dtype: {self.dtype}\")\n",
    "\n",
    "        # optimize model for inference\n",
    "        self._model = self._model.eval().to(device=self.device, dtype=self.dtype)\n",
    "        self._model = self._model.to(memory_format=torch.channels_last)\n",
    "        self._model = torch.compile(self._model)\n",
    "        \n",
    "\n",
    "    def prepare(self, image: torch.Tensor | Image.Image) -> torch.Tensor:\n",
    "         \"\"\"transform image\"\"\"\n",
    "         return self._transform(image).unsqueeze(0).to(self.device, self.dtype,\n",
    "                                         memory_format=torch.channels_last)\n",
    "\n",
    "\n",
    "    def embed(self, img_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"embed tensor of already-transformed images\"\"\"\n",
    "        with self._amp_autocast():\n",
    "            return self._model(img_tensor).detach().cpu().float()\n",
    "\n",
    "            \n",
    "    def __call__(self, image: torch.Tensor | Image.Image) -> np.array:\n",
    "        \"\"\"transform image, run inference, extract embedding as 1D array\"\"\"\n",
    "        image = self.prepare(image)\n",
    "        return self.embed(image)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference device: cpu with dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "# it's a one-stop shop\n",
    "e = TimmEmbedder(\"mobilenetv3_large_100\")\n",
    "e(torch.rand((3,256,256)))\n",
    "\n",
    "# or do in two steps\n",
    "\n",
    "t = e.prepare(torch.rand((3,256,256)))\n",
    "v = e.embed(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@daft.udf(return_dtype=daft.DataType.list(daft.DataType.float32()),\n",
    "          memory_bytes=MEMORY_BYTES)\n",
    "class EmbedImages:\n",
    "    \"\"\"run timm embedder on an image column\"\"\"\n",
    "    def __init__(self, model_name: str, batch_size: int = 4):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.embedder = TimmEmbedder(self.model_name)\n",
    "        \n",
    "    def __call__(self, images_col) -> list:\n",
    "        images = [rearrange(im, \"h w c -> c h w\") for im in images_col.to_pylist()]\n",
    "        images = torch.cat([self.embedder.prepare(torch.tensor(im,\n",
    "                                dtype=self.embedder.dtype, device=self.embedder.device)\n",
    "                            ) for im in images],\n",
    "                           dim=0)\n",
    "        ## Example: https://colab.research.google.com/github/Eventual-Inc/Daft/blob/main/tutorials/mnist.ipynb\n",
    "        return torch.cat([self.embedder.embed(batch) for batch in torch.split(\n",
    "                            images, self.batch_size, dim=0)],\n",
    "                        dim=0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: I need to figure out how to create two udfs that work together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/willsa/git/bedmap-dev/.venv/lib/python3.12/site-packages/daft/context.py:168: UserWarning: Daft is configured to use the new NativeRunner by default as of v0.4.0. If you are encountering any regressions, please switch back to the legacy PyRunner via `daft.context.set_runner_py()` or by setting the env variable `DAFT_RUNNER=py`. We appreciate you filing issues and helping make the NativeRunner better: https://github.com/Eventual-Inc/Daft/issues\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "glob = tmp_path.as_posix() +\"/*.png\"\n",
    "images_df = daft.from_glob_path(glob).with_column_renamed(\"path\", \"path_full_img\")\n",
    "\n",
    "images_df = images_df.with_column(\"image\", daft.col(\"path_full_img\"\n",
    "                                 ).url.download().image.decode(\n",
    "                                     mode=\"RGB\", on_error=\"null\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "EmbedImagesWithModel = EmbedImages.with_init_args(model_name=MODEL_NAME, batch_size=BATCH_SIZE)\n",
    "images_df = images_df.with_column(\"embed\", EmbedImagesWithModel(daft.col(\"image\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7082cde36c4206b4146b3f326dd69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üó°Ô∏è üêü Project: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "images_df = images_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "shutil.rmtree(tmp_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
