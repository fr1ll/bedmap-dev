{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import shutil\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "import daft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "MODEL_NAME = \"vit_base_patch14_reg4_dinov2.lvd142m\"\n",
    "IMAGE_GLOB = None\n",
    "IMAGES_FOLDER = \"./tmp-test-images\"\n",
    "\n",
    "TEST_DATASET = \"kvriza8/microscopy_images\"\n",
    "NUM_TEST_IMAGES = 500\n",
    "\n",
    "nice_models = [\n",
    "\"mobilenetv3_large_100\",\n",
    "\"vit_small_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_large_patch14_reg4_dinov2.lvd142m\",\n",
    "\"aimv2_large_patch14_224.apple_pt_dist\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with vit_base_patch14 and torch dataloader:\n",
    "\n",
    "num_images | batch_size | optimize | time |\n",
    "-----------|------------|----------|------|\n",
    "500        |         32 | False    | 10:07 \n",
    "200        |         32 | False    | 04:50 \n",
    "2000       |         32 | False    | 41:00 \n",
    "50         |         32 | Static   | 01:14\n",
    "50         |         32 | Dynamic  | 01:14\n",
    "500        |         32 | Static   | 09:33 \n",
    "2000       | 32 (fixed) | Static   | 36:22\n",
    "2000       | 16 (fixed) | Static   | 36:32\n",
    "2000       |  4 (fixed) | Static   | 39:17\n",
    "2000       | 128 (fixd) | Static   | OOM\n",
    "2000lrg    |  16 (fixd) | Static   | -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_hf_images(dataset_name: str = \"kvriza8/microscopy_images\",\n",
    "                 dir: Path = None, max_images: int = 50) -> None:\n",
    "    dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "\n",
    "    for i, img_row in enumerate(tqdm(iter(dataset), total=max_images)):\n",
    "        if i >= max_images:\n",
    "            break\n",
    "        img = img_row[\"image\"]\n",
    "        img.save(dir / f\"{i}.png\")\n",
    "\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:10<00:00, 46.84it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "tmp_path = Path(IMAGES_FOLDER)\n",
    "shutil.rmtree(tmp_path, ignore_errors=True)\n",
    "tmp_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dl_hf_images(dir=tmp_path, max_images=NUM_TEST_IMAGES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Embedder:\n",
    "    model_name: str\n",
    "    device: torch.device = field(default_factory=lambda: torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model: torch.nn.Module = field(init=False)\n",
    "    transform: callable = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.model = timm.create_model(self.model_name, pretrained=True, num_classes=0)\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device, memory_format=torch.channels_last)\n",
    "        self.model = torch.compile(self.model, dynamic=True)\n",
    "        # Resolve config removes unneeded fields before create_transform\n",
    "        cfg = timm.data.resolve_data_config(self.model.pretrained_cfg, model=self.model)\n",
    "        self.transform = timm.data.create_transform(**cfg)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def embed(self, batch_imgs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Given a batch of pre-transformed images, compute pooled embeddings.\n",
    "        The batch is moved to the proper device (with channels_last format) and processed in inference mode.\n",
    "        \"\"\"\n",
    "        batch_imgs = batch_imgs.to(self.device, non_blocking=True)\n",
    "        batch_imgs = batch_imgs.contiguous(memory_format=torch.channels_last)\n",
    "        if self.device.type == \"cuda\":\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                return self.model(batch_imgs)\n",
    "        else:\n",
    "            # autocast can be comically slow for some CPU setups (PyTorch issue #118499)\n",
    "            return self.model(batch_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@daft.udf(return_dtype=daft.DataType.python())\n",
    "class TransformImageCol:\n",
    "    \"\"\"run timm embedder on an image column\"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.embedder = Embedder(self.model_name)\n",
    "\n",
    "    def __call__(self, batch_images) -> list:\n",
    "        return [self.embedder.transform(Image.fromarray(im)) for im in batch_images.to_pylist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table class=\"dataframe\">\n",
       "<thead><tr><th style=\"text-wrap: nowrap; max-width:192px; overflow:auto; text-align:left\">path_full_img<br />Utf8</th><th style=\"text-wrap: nowrap; max-width:192px; overflow:auto; text-align:left\">size<br />Int64</th><th style=\"text-wrap: nowrap; max-width:192px; overflow:auto; text-align:left\">image_transformed<br />Python</th></tr></thead>\n",
       "<tbody>\n",
       "<tr><td><div style=\"text-align:left; max-width:192px; max-height:64px; overflow:auto\">file://tmp-test-images/60.png</div></td><td><div style=\"text-align:left; max-width:192px; max-height:64px; overflow:auto\">37758</div></td><td><div style=\"text-align:left; max-width:192px; max-height:64px; overflow:auto\">tensor([[[-0.7993, -0.7308, -0.6794,  ...,  0.6734,  1.5810,  2.2489],<br />         [-0.9363, -0.8678, -0.8164,  ...,  0.6049,  1.5468,  2.2489],<br />         [-1.2617, -1.2103, -1.1589,  ...,  0.4851,  1.4783,  2.2318],<br />         ...,<br />         [-0.4739, -0.5596, -0.6623,  ...,  0.3309,  1.4269,  2.2489],<br />         [-0.7822, -0.9363, -1.0219,  ...,  0.3309,  1.4269,  2.2489],<br />         [-0.9363, -1.1075, -1.2103,  ...,  0.3309,  1.4269,  2.2489]],<br /><br />        [[-0.6877, -0.6176, -0.5651,  ...,  0.8179,  1.7458,  2.4286],<br />         [-0.8277, -0.7577, -0.7052,  ...,  0.7479,  1.7108,  2.4286],<br />         [-1.1604, -1.1078, -1.0553,  ...,  0.6254,  1.6408,  2.4111],<br />         ...,<br />         [-0.3550, -0.4426, -0.5476,  ...,  0.4678,  1.5882,  2.4286],<br />         [-0.6702, -0.8277, -0.9153,  ...,  0.4678,  1.5882,  2.4286],<br />         [-0.8277, -1.0028, -1.1078,  ...,  0.4678,  1.5882,  2.4286]],<br /><br />        [[-0.4624, -0.3927, -0.3404,  ...,  1.0365,  1.9603,  2.6400],<br />         [-0.6018, -0.5321, -0.4798,  ...,  0.9668,  1.9254,  2.6400],<br />         [-0.9330, -0.8807, -0.8284,  ...,  0.8448,  1.8557,  2.6226],<br />         ...,<br />         [-0.1312, -0.2184, -0.3230,  ...,  0.6879,  1.8034,  2.6400],<br />         [-0.4450, -0.6018, -0.6890,  ...,  0.6879,  1.8034,  2.6400],<br />         [-0.6018, -0.7761, -0.8807,  ...,  0.6879,  1.8034,  2.6400]]])</div></td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<small>(Showing first 1 rows)</small>\n",
       "</div>"
      ],
      "text/plain": [
       "╭───────────────────────────────┬───────┬────────────────────────────────╮\n",
       "│ path_full_img                 ┆ size  ┆ image_transformed              │\n",
       "│ ---                           ┆ ---   ┆ ---                            │\n",
       "│ Utf8                          ┆ Int64 ┆ Python                         │\n",
       "╞═══════════════════════════════╪═══════╪════════════════════════════════╡\n",
       "│ file://tmp-test-images/60.png ┆ 37758 ┆ tensor([[[-0.7993, -0.7308, -… │\n",
       "╰───────────────────────────────┴───────┴────────────────────────────────╯\n",
       "\n",
       "(Showing first 1 rows)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imglob = tmp_path.as_posix() +\"/*.png\"\n",
    "images_df = daft.from_glob_path(imglob).with_column_renamed(\"path\", \"path_full_img\")\n",
    "images_df = images_df.with_column(\"image\", daft.col(\"path_full_img\"\n",
    "                                 ).url.download().image.decode(\n",
    "                                     mode=\"RGB\", on_error=\"null\")\n",
    "                                 )\n",
    "images_df = images_df.where(images_df[\"image\"].not_null())\n",
    "\n",
    "TransformImForModel = TransformImageCol.with_init_args(model_name=MODEL_NAME)\n",
    "\n",
    "images_df = images_df.with_column(\"image_transformed\", TransformImForModel(daft.col(\"image\"))\n",
    "                                  ).exclude(\"image\", \"num_rows\")\n",
    "\n",
    "images_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_embeddings(model_name:\n",
    "                       str, dataset: torch.utils.data.IterableDataset,\n",
    "                       batch_size: int = BATCH_SIZE) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Given a model name and a filelist (list of image paths), this function computes and returns a list\n",
    "    of embeddings (one per image). The function instantiates an Embedder, builds a dataset and dataloader,\n",
    "    and processes images in batches.\n",
    "    \"\"\"\n",
    "    embedder = Embedder(model_name=model_name)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    for i, batch_images in enumerate(tqdm(dataloader, unit_scale=BATCH_SIZE)):\n",
    "        emb = embedder.embed(batch_images[\"image_transformed\"]).cpu().numpy()\n",
    "\n",
    "        if i == 0:\n",
    "            embeddings = emb\n",
    "            print(f\"Shape of embedding for one batch: {emb.shape}\")\n",
    "        else:\n",
    "            embeddings = np.concatenate((embeddings, emb), axis=0)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "images_dataset = images_df.to_torch_iter_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf15f0773d54e189245c006bb17e42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "🗡️ 🐟 Project: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3670d7de0247578be8016b861f8eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "🗡️ 🐟 Filter: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c0babb9e32439585eabb784db7a50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "🗡️ 🐟 Project: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:47,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding for one batch: (32, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "480it [08:50,  1.07s/it][W227 14:03:37.814903742 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "512it [09:12,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 500 embeddings.\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference         0.04%     224.783ms       100.00%      553.379s      553.379s           0 b      -3.33 Gb             1  \n",
      "                             Torch-Compiled Region: 0/0         3.53%       19.516s        96.88%      536.134s       33.508s       1.46 Mb     -25.91 Gb            16  \n",
      "                                            aten::addmm        64.72%      358.138s        68.68%      380.043s     395.878ms           0 b           0 b           960  \n",
      "      aten::_scaled_dot_product_flash_attention_for_cpu        24.45%      135.286s        24.47%      135.398s     490.574ms      23.95 Gb    -867.00 Mb           276  \n",
      "                                            aten::copy_         3.92%       21.719s         3.92%       21.719s      19.375ms           0 b           0 b          1121  \n",
      "                  _compile.compile_inner (dynamo_timed)         0.30%        1.661s         1.67%        9.217s        9.217s           0 b           0 b             1  \n",
      "          OutputGraph.call_user_compiler (dynamo_timed)         0.01%      38.211ms         1.21%        6.715s        6.715s           0 b           0 b             1  \n",
      "          create_aot_dispatcher_function (dynamo_timed)         0.08%     435.362ms         1.20%        6.657s        6.657s           0 b      -4.94 Kb             1  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...         1.03%        5.716s         1.19%        6.587s     387.475ms       1.50 Gb           0 b            17  \n",
      "    compile_fx.<locals>.fw_compiler_base (dynamo_timed)         0.00%     750.374us         0.54%        3.015s        3.015s           0 b           0 b             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 553.379s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], profile_memory=True, record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "\n",
    "        embeddings = compute_embeddings(MODEL_NAME, images_dataset, BATCH_SIZE)\n",
    "\n",
    "# print(f\"Processed {len(images_dataset.count_rows())} images.\")\n",
    "print(f\"Got {len(embeddings)} embeddings.\")\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.export_chrome_trace(f\"daftiter_trace_{MODEL_NAME}_{NUM_TEST_IMAGES}x{BATCH_SIZE}.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
