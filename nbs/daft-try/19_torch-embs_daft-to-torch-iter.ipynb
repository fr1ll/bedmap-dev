{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "import daft\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 8\n",
    "MODEL_NAME = \"vit_large_patch14_reg4_dinov2.lvd142m\"\n",
    "IMAGE_GLOB = None\n",
    "IMAGES_FOLDER = \"./tmp-test-images\"\n",
    "\n",
    "TEST_DATASET = \"kvriza8/microscopy_images\"\n",
    "NUM_TEST_IMAGES = 50\n",
    "\n",
    "nice_models = [\n",
    "\"mobilenetv3_large_100\",\n",
    "\"vit_small_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_large_patch14_reg4_dinov2.lvd142m\",\n",
    "\"aimv2_large_patch14_224.apple_pt_dist\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with vit_base_patch14 and torch dataloader:\n",
    "\n",
    "num_images | batch_size | optimize | time |\n",
    "-----------|------------|----------|------|\n",
    "500        |         32 | False    | 10:07 \n",
    "200        |         32 | False    | 04:50 \n",
    "2000       |         32 | False    | 41:00 \n",
    "50         |         32 | Static   | 01:14\n",
    "50         |         32 | Dynamic  | 01:14\n",
    "500        |         32 | Static   | 09:33 \n",
    "2000       | 32 (fixed) | Static   | 36:22\n",
    "2000       | 16 (fixed) | Static   | 36:32\n",
    "2000       |  4 (fixed) | Static   | 39:17\n",
    "2000       | 128 (fixd) | Static   | OOM\n",
    "2000lrg    |  16 (fixd) | Static   | -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_hf_images(dataset_name: str = \"kvriza8/microscopy_images\",\n",
    "                 dir: Path = None, max_images: int = 50) -> None:\n",
    "    dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "\n",
    "    for i, img_row in enumerate(tqdm(iter(dataset), total=max_images)):\n",
    "        if i >= max_images:\n",
    "            break\n",
    "        img = img_row[\"image\"]\n",
    "        img.save(dir / f\"{i}.png\")\n",
    "\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.90it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tmp_path = Path(IMAGES_FOLDER)\n",
    "shutil.rmtree(tmp_path, ignore_errors=True)\n",
    "tmp_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dl_hf_images(dir=tmp_path, max_images=NUM_TEST_IMAGES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Embedder:\n",
    "    model_name: str\n",
    "    device: torch.device = field(default_factory=lambda: torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model: torch.nn.Module = field(init=False)\n",
    "    transform: callable = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.model = timm.create_model(self.model_name, pretrained=True, num_classes=0)\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device, memory_format=torch.channels_last)\n",
    "        self.model = torch.compile(self.model, dynamic=True)\n",
    "        # Resolve config removes unneeded fields before create_transform\n",
    "        cfg = timm.data.resolve_data_config(self.model.pretrained_cfg, model=self.model)\n",
    "        self.transform = timm.data.create_transform(**cfg)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def embed(self, batch_imgs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Given a batch of pre-transformed images, compute pooled embeddings.\n",
    "        The batch is moved to the proper device (with channels_last format) and processed in inference mode.\n",
    "        \"\"\"\n",
    "        batch_imgs = batch_imgs.to(self.device, non_blocking=True)\n",
    "        batch_imgs = batch_imgs.contiguous(memory_format=torch.channels_last)\n",
    "        if self.device.type == \"cuda\":\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                return self.model(batch_imgs)\n",
    "        else:\n",
    "            # autocast can be comically slow for some CPU setups (PyTorch issue #118499)\n",
    "            return self.model(batch_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@daft.udf(return_dtype=daft.DataType.python())\n",
    "class TransformImageCol:\n",
    "    \"\"\"run timm embedder on an image column\"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.embedder = Embedder(self.model_name)\n",
    "\n",
    "    def __call__(self, batch_images) -> list:\n",
    "        return [self.embedder.transform(Image.fromarray(im)) for im in batch_images.to_pylist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table class=\"dataframe\">\n",
       "<thead><tr><th style=\"text-wrap: nowrap; max-width:192px; overflow:auto; text-align:left\">path_full_img<br />Utf8</th><th style=\"text-wrap: nowrap; max-width:192px; overflow:auto; text-align:left\">size<br />Int64</th><th style=\"text-wrap: nowrap; max-width:192px; overflow:auto; text-align:left\">image_transformed<br />Python</th></tr></thead>\n",
       "<tbody>\n",
       "<tr><td><div style=\"text-align:left; max-width:192px; max-height:64px; overflow:auto\">file://tmp-test-images/43.png</div></td><td><div style=\"text-align:left; max-width:192px; max-height:64px; overflow:auto\">52246</div></td><td><div style=\"text-align:left; max-width:192px; max-height:64px; overflow:auto\">tensor([[[ 2.0263,  1.9749,  1.8550,  ...,  1.8037,  1.8208,  1.8208],<br />         [ 1.9920,  1.9064,  1.6495,  ...,  1.2728,  1.2899,  1.2899],<br />         [ 1.8550,  1.7009,  1.3070,  ...,  0.6563,  0.6563,  0.6563],<br />         ...,<br />         [ 1.1015,  0.7419, -0.1486,  ..., -0.6623, -0.6965, -0.6965],<br />         [ 1.2557,  0.8961,  0.0056,  ..., -0.6965, -0.7308, -0.7308],<br />         [ 1.3413,  0.9817,  0.0912,  ..., -0.7822, -0.7993, -0.7993]],<br /><br />        [[ 2.3761,  2.3410,  2.2360,  ...,  2.2535,  2.2535,  2.2535],<br />         [ 2.4286,  2.3761,  2.1310,  ...,  1.8158,  1.8158,  1.8158],<br />         [ 2.3936,  2.2885,  1.8859,  ...,  1.2731,  1.2731,  1.2731],<br />         ...,<br />         [ 2.1660,  1.7808,  0.8354,  ..., -0.3200, -0.3025, -0.3025],<br />         [ 2.3235,  1.9209,  0.9930,  ..., -0.3025, -0.2850, -0.2850],<br />         [ 2.3761,  1.9909,  1.0805,  ..., -0.2850, -0.2850, -0.2850]],<br /><br />        [[ 2.6400,  2.6400,  2.6051,  ...,  2.5180,  2.5180,  2.5180],<br />         [ 2.6400,  2.6226,  2.5354,  ...,  2.2217,  2.2217,  2.2217],<br />         [ 2.6400,  2.5877,  2.3611,  ...,  1.7860,  1.7860,  1.7860],<br />         ...,<br />         [ 2.6400,  2.4483,  1.5245,  ...,  0.1302,  0.1476,  0.1476],<br />         [ 2.6400,  2.4657,  1.6117,  ...,  0.1825,  0.1999,  0.1999],<br />         [ 2.6400,  2.4831,  1.6814,  ...,  0.2348,  0.2522,  0.2522]]])</div></td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<small>(Showing first 1 rows)</small>\n",
       "</div>"
      ],
      "text/plain": [
       "╭───────────────────────────────┬───────┬────────────────────────────────╮\n",
       "│ path_full_img                 ┆ size  ┆ image_transformed              │\n",
       "│ ---                           ┆ ---   ┆ ---                            │\n",
       "│ Utf8                          ┆ Int64 ┆ Python                         │\n",
       "╞═══════════════════════════════╪═══════╪════════════════════════════════╡\n",
       "│ file://tmp-test-images/43.png ┆ 52246 ┆ tensor([[[ 2.0263,  1.9749,  … │\n",
       "╰───────────────────────────────┴───────┴────────────────────────────────╯\n",
       "\n",
       "(Showing first 1 rows)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imglob = tmp_path.as_posix() +\"/*.png\"\n",
    "images_df = daft.from_glob_path(imglob).with_column_renamed(\"path\", \"path_full_img\")\n",
    "images_df = images_df.with_column(\"image\", daft.col(\"path_full_img\"\n",
    "                                 ).url.download().image.decode(\n",
    "                                     mode=\"RGB\", on_error=\"null\")\n",
    "                                 )\n",
    "images_df = images_df.where(images_df[\"image\"].not_null())\n",
    "\n",
    "TransformImForModel = TransformImageCol.with_init_args(model_name=MODEL_NAME)\n",
    "\n",
    "images_df = images_df.with_column(\"image_transformed\", TransformImForModel(daft.col(\"image\"))\n",
    "                                  ).exclude(\"image\", \"num_rows\")\n",
    "\n",
    "images_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_to_batch_size_from_dict(batch_dict, batch_size):\n",
    "    \"\"\"Pads the batch to batch_size with zeros\"\"\"\n",
    "    batch = batch_dict[\"image_transformed\"]\n",
    "    orig_size = len(batch)\n",
    "    batch = torch.stack(batch)\n",
    "    if orig_size < batch_size:\n",
    "        pad_tensor = torch.zeros((batch_size - orig_size, *batch.shape[1:]),\n",
    "                                 dtype=batch.dtype, device=batch.device)\n",
    "        batch = torch.cat([batch, pad_tensor], dim=0)\n",
    "    return batch, orig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_embeddings(model_name:\n",
    "                       str, dataset: torch.utils.data.IterableDataset,\n",
    "                       batch_size: int = BATCH_SIZE) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Given a model name and a filelist (list of image paths), this function computes and returns a list\n",
    "    of embeddings (one per image). The function instantiates an Embedder, builds a dataset and dataloader,\n",
    "    and processes images in batches.\n",
    "    \"\"\"\n",
    "    embedder = Embedder(model_name=model_name)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=lambda b: pad_to_batch_size_from_dict(b, batch_size),\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    for i, (batch_imgs, actual_batch_size) in enumerate(tqdm(dataloader)):\n",
    "        emb = embedder.embed(batch_imgs).cpu().numpy()\n",
    "        emb = emb[:actual_batch_size, ...]\n",
    "\n",
    "        if i == 0:\n",
    "            embeddings = emb\n",
    "            print(f\"Shape of embedding for one batch: {emb.shape}\")\n",
    "        else:\n",
    "            embeddings = np.concatenate((embeddings, emb), axis=0)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "images_dataset = images_df.to_torch_iter_dataset()\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], profile_memory=True, record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "\n",
    "        embeddings = compute_embeddings(MODEL_NAME, images_dataset, BATCH_SIZE)\n",
    "\n",
    "# print(f\"Processed {len(images_dataset.count_rows())} images.\")\n",
    "print(f\"Got {len(embeddings)} embeddings.\")\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: single '}' is not allowed (965503420.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    prof.export_chrome_trace(f\"trace_{MODEL_NAME}_{NUM_TEST_IMAGES}}x{BATCH_SIZE}.json\")\u001b[0m\n\u001b[0m                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: single '}' is not allowed\n"
     ]
    }
   ],
   "source": [
    "prof.export_chrome_trace(f\"daftiter_trace_{MODEL_NAME}_{NUM_TEST_IMAGES}x{BATCH_SIZE}.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
