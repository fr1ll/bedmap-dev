{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import shutil\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "import daft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 64\n",
    "MODEL_NAME = \"vit_base_patch14_reg4_dinov2.lvd142m\"\n",
    "IMAGE_GLOB = None\n",
    "IMAGES_FOLDER = \"./tmp-test-images\"\n",
    "\n",
    "TEST_DATASET = \"kvriza8/microscopy_images\"\n",
    "NUM_TEST_IMAGES = 2000\n",
    "\n",
    "nice_models = [\n",
    "\"mobilenetv3_large_100\",\n",
    "\"vit_small_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_large_patch14_reg4_dinov2.lvd142m\",\n",
    "\"aimv2_large_patch14_224.apple_pt_dist\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with vit_base_patch14 and torch dataloader:\n",
    "\n",
    "num_images | batch_size | optimize | time |\n",
    "-----------|------------|----------|------|\n",
    "500        |         32 | False    | 10:07 \n",
    "200        |         32 | False    | 04:50 \n",
    "2000       |         32 | False    | 41:00 \n",
    "50         |         32 | Static   | 01:14\n",
    "50         |         32 | Dynamic  | 01:14\n",
    "500        |         32 | Static   | 09:33 \n",
    "2000       | 32 (fixed) | Static   | 36:22\n",
    "2000       | 16 (fixed) | Static   | 36:32\n",
    "2000       |  4 (fixed) | Static   | 39:17\n",
    "2000       | 128 (fixd) | Static   | OOM\n",
    "2000lrg    |  16 (fixd) | Static   | -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_hf_images(dataset_name: str = \"kvriza8/microscopy_images\",\n",
    "                 dir: Path = None, max_images: int = 50) -> None:\n",
    "    dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "\n",
    "    for i, img_row in enumerate(tqdm(iter(dataset), total=max_images)):\n",
    "        if i >= max_images:\n",
    "            break\n",
    "        img = img_row[\"image\"]\n",
    "        img.save(dir / f\"{i}.png\")\n",
    "\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:38<00:00, 51.76it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "tmp_path = Path(IMAGES_FOLDER)\n",
    "shutil.rmtree(tmp_path, ignore_errors=True)\n",
    "tmp_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dl_hf_images(dir=tmp_path, max_images=NUM_TEST_IMAGES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Embedder:\n",
    "    model_name: str\n",
    "    device: torch.device = field(default_factory=lambda: torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model: torch.nn.Module = field(init=False)\n",
    "    transform: callable = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.model = timm.create_model(self.model_name, pretrained=True, num_classes=0)\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device, memory_format=torch.channels_last)\n",
    "        self.model = torch.compile(self.model, dynamic=True)\n",
    "        # Resolve config removes unneeded fields before create_transform\n",
    "        cfg = timm.data.resolve_data_config(self.model.pretrained_cfg, model=self.model)\n",
    "        self.transform = timm.data.create_transform(**cfg)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def embed(self, batch_imgs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Given a batch of pre-transformed images, compute pooled embeddings.\n",
    "        The batch is moved to the proper device (with channels_last format) and processed in inference mode.\n",
    "        \"\"\"\n",
    "        batch_imgs = batch_imgs.to(self.device, non_blocking=True)\n",
    "        batch_imgs = batch_imgs.contiguous(memory_format=torch.channels_last)\n",
    "        if self.device.type == \"cuda\":\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                return self.model(batch_imgs)\n",
    "        else:\n",
    "            # autocast can be comically slow for some CPU setups (PyTorch issue #118499)\n",
    "            return self.model(batch_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@daft.udf(return_dtype=daft.DataType.python())\n",
    "class TransformImageCol:\n",
    "    \"\"\"run timm embedder on an image column\"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.embedder = Embedder(self.model_name)\n",
    "\n",
    "    def __call__(self, batch_images) -> list:\n",
    "        return [self.embedder.transform(Image.fromarray(im)) for im in batch_images.to_pylist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table class=\"dataframe\">\n",
       "<thead><tr><th style=\"text-wrap: nowrap; max-width:192px; overflow:auto; text-align:left\">path_full_img<br />Utf8</th><th style=\"text-wrap: nowrap; max-width:192px; overflow:auto; text-align:left\">size<br />Int64</th><th style=\"text-wrap: nowrap; max-width:192px; overflow:auto; text-align:left\">image_transformed<br />Python</th></tr></thead>\n",
       "<tbody>\n",
       "<tr><td><div style=\"text-align:left; max-width:192px; max-height:64px; overflow:auto\">file://tmp-test-images/1354.png</div></td><td><div style=\"text-align:left; max-width:192px; max-height:64px; overflow:auto\">40149</div></td><td><div style=\"text-align:left; max-width:192px; max-height:64px; overflow:auto\">tensor([[[-1.2959, -1.2617, -1.2103,  ..., -1.8268, -1.8268, -1.8268],<br />         [-1.3302, -1.2445, -1.1589,  ..., -1.8610, -1.8610, -1.8439],<br />         [-1.3130, -1.2445, -1.1932,  ..., -1.8610, -1.8610, -1.8439],<br />         ...,<br />         [ 1.3242,  1.1872,  0.8789,  ..., -0.3883, -0.3369, -0.2856],<br />         [ 1.1872,  1.0502,  0.7933,  ..., -0.2856, -0.2684, -0.2342],<br />         [ 1.2557,  0.9988,  0.5878,  ..., -0.0972, -0.0972, -0.1486]],<br /><br />        [[-0.9853, -0.9503, -0.9678,  ..., -1.6856, -1.6681, -1.6506],<br />         [-0.9503, -0.8627, -0.8277,  ..., -1.6856, -1.6681, -1.6506],<br />         [-0.9153, -0.8452, -0.8452,  ..., -1.6856, -1.6681, -1.6506],<br />         ...,<br />         [ 1.3606,  1.1506,  0.8529,  ..., -0.3550, -0.3025, -0.2500],<br />         [ 1.2206,  1.0105,  0.7654,  ..., -0.2675, -0.2500, -0.2150],<br />         [ 1.2906,  0.9755,  0.5553,  ..., -0.0924, -0.0749, -0.1275]],<br /><br />        [[-1.0376, -1.0550, -1.0201,  ..., -1.3339, -1.3164, -1.3164],<br />         [-1.0201, -0.9678, -0.8807,  ..., -1.3513, -1.3339, -1.2990],<br />         [-0.9853, -0.9504, -0.8807,  ..., -1.3687, -1.3513, -1.3164],<br />         ...,<br />         [ 1.4374,  1.2805,  1.0888,  ..., -0.1487, -0.0790, -0.0441],<br />         [ 1.3154,  1.1411,  0.9842,  ..., -0.0267, -0.0267,  0.0082],<br />         [ 1.4025,  1.1237,  0.7576,  ...,  0.1651,  0.1651,  0.0953]]])</div></td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<small>(Showing first 1 rows)</small>\n",
       "</div>"
      ],
      "text/plain": [
       "╭────────────────────────────────┬───────┬────────────────────────────────╮\n",
       "│ path_full_img                  ┆ size  ┆ image_transformed              │\n",
       "│ ---                            ┆ ---   ┆ ---                            │\n",
       "│ Utf8                           ┆ Int64 ┆ Python                         │\n",
       "╞════════════════════════════════╪═══════╪════════════════════════════════╡\n",
       "│ file://tmp-test-images/1354.p… ┆ 40149 ┆ tensor([[[-1.2959, -1.2617, -… │\n",
       "╰────────────────────────────────┴───────┴────────────────────────────────╯\n",
       "\n",
       "(Showing first 1 rows)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imglob = tmp_path.as_posix() +\"/*.png\"\n",
    "images_df = daft.from_glob_path(imglob).with_column_renamed(\"path\", \"path_full_img\")\n",
    "images_df = images_df.with_column(\"image\", daft.col(\"path_full_img\"\n",
    "                                 ).url.download().image.decode(\n",
    "                                     mode=\"RGB\", on_error=\"null\")\n",
    "                                 )\n",
    "images_df = images_df.where(images_df[\"image\"].not_null())\n",
    "\n",
    "TransformImForModel = TransformImageCol.with_init_args(model_name=MODEL_NAME)\n",
    "\n",
    "images_df = images_df.with_column(\"image_transformed\", TransformImForModel(daft.col(\"image\"))\n",
    "                                  ).exclude(\"image\", \"num_rows\")\n",
    "\n",
    "images_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_embeddings(model_name:\n",
    "                       str, dataset: torch.utils.data.IterableDataset,\n",
    "                       batch_size: int = BATCH_SIZE) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Given a model name and a filelist (list of image paths), this function computes and returns a list\n",
    "    of embeddings (one per image). The function instantiates an Embedder, builds a dataset and dataloader,\n",
    "    and processes images in batches.\n",
    "    \"\"\"\n",
    "    embedder = Embedder(model_name=model_name)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    for i, batch_images in enumerate(tqdm(dataloader, unit_scale=BATCH_SIZE)):\n",
    "        emb = embedder.embed(batch_images[\"image_transformed\"]).cpu().numpy()\n",
    "\n",
    "        if i == 0:\n",
    "            embeddings = emb\n",
    "            print(f\"Shape of embedding for one batch: {emb.shape}\")\n",
    "        else:\n",
    "            embeddings = np.concatenate((embeddings, emb), axis=0)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "images_dataset = images_df.to_torch_iter_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b016291018bf46ada175b996b8446334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "🗡️ 🐟 Project: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7357237f489644a18505691953b238ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "🗡️ 🐟 Filter: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3d7f62ef5943c99cd2dadcae7feee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "🗡️ 🐟 Project: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:30,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding for one batch: (64, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1984it [00:43, 156.16it/s][W227 12:39:47.181416818 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "[W227 12:39:47.184368241 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "2048it [00:43, 47.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 2000 embeddings.\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference         0.54%     234.886ms       100.00%       43.286s       43.286s           0 b      -2.28 Gb             1  \n",
      "                  _compile.compile_inner (dynamo_timed)         6.74%        2.916s        51.17%       22.149s       22.149s           0 b           0 b             1  \n",
      "          OutputGraph.call_user_compiler (dynamo_timed)         0.15%      66.495ms        31.39%       13.587s       13.587s           0 b           0 b             1  \n",
      "          create_aot_dispatcher_function (dynamo_timed)         1.81%     782.770ms        31.18%       13.498s       13.498s           0 b      -4.94 Kb             1  \n",
      "                                      aten::convolution         3.32%        1.436s        30.48%       13.192s       5.369ms      32.83 Gb           0 b          2457  \n",
      "                             Torch-Compiled Region: 0/0         8.94%        3.869s        27.72%       11.999s     374.955ms       9.77 Mb     -32.82 Gb            32  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...        18.30%        7.921s        19.84%        8.590s     260.294ms       1.12 Gb           0 b            33  \n",
      "                                     aten::_convolution         0.13%      56.771ms        18.70%        8.095s       4.016ms      32.83 Gb           0 b          2016  \n",
      "                               aten::mkldnn_convolution        18.40%        7.967s        18.57%        8.038s       3.987ms      32.83 Gb           0 b          2016  \n",
      "                                       aten::batch_norm         0.06%      25.326ms        15.54%        6.725s      48.735ms           0 b           0 b           138  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 43.286s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], profile_memory=True, record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "\n",
    "        embeddings = compute_embeddings(MODEL_NAME, images_dataset, BATCH_SIZE)\n",
    "\n",
    "# print(f\"Processed {len(images_dataset.count_rows())} images.\")\n",
    "print(f\"Got {len(embeddings)} embeddings.\")\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.export_chrome_trace(f\"daftiter_trace_{MODEL_NAME}_{NUM_TEST_IMAGES}x{BATCH_SIZE}.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
