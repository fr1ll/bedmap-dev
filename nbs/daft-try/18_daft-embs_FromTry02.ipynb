{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp daft_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import gc\n",
    "import shutil\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import daft\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from einops import rearrange\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MODEL_NAME = \"vit_large_patch14_reg4_dinov2.lvd142m\"\n",
    "IMAGE_GLOB = None\n",
    "IMAGES_FOLDER = \"./tmp-test-images\"\n",
    "\n",
    "TEST_DATASET = \"kvriza8/microscopy_images\"\n",
    "NUM_TEST_IMAGES = 2_000\n",
    "\n",
    "MEMORY_BYTES = int(6e9) # 6 GB memory allocation\n",
    "daft.set_execution_config(enable_native_executor=True,\n",
    "                          default_morsel_size=BATCH_SIZE\n",
    "                          )\n",
    "\n",
    "nice_models = [\n",
    "\"mobilenetv3_large_100\",\n",
    "\"vit_small_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_large_patch14_reg4_dinov2.lvd142m\",\n",
    "\"aimv2_large_patch14_224.apple_pt_dist\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\n",
    "\"mobilenetv3_large_100\",\n",
    "\"vit_small_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_large_patch14_reg4_dinov2.lvd142m\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_hf_images(dataset_name: str = \"kvriza8/microscopy_images\",\n",
    "                 dir: Path = None, max_images: int = 50) -> None:\n",
    "    dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "\n",
    "    for i, img_row in enumerate(tqdm(iter(dataset), total=max_images)):\n",
    "        if i >= max_images:\n",
    "            break\n",
    "        img = img_row[\"image\"]\n",
    "        img.save(dir / f\"{i}.png\")\n",
    "\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:35<00:00, 55.73it/s] \n"
     ]
    }
   ],
   "source": [
    "tmp_path = Path(IMAGES_FOLDER)\n",
    "shutil.rmtree(tmp_path, ignore_errors=True)\n",
    "tmp_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dl_hf_images(dir=tmp_path, max_images=NUM_TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Embedder:\n",
    "    model_name: str\n",
    "    device: torch.device = field(default_factory=lambda: torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model: torch.nn.Module = field(init=False)\n",
    "    transform: callable = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.model = timm.create_model(self.model_name, pretrained=True, num_classes=0)\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device, memory_format=torch.channels_last)\n",
    "        self.model = torch.compile(self.model, dynamic=True)\n",
    "        # Resolve config removes unneeded fields before create_transform\n",
    "        cfg = timm.data.resolve_data_config(self.model.pretrained_cfg, model=self.model)\n",
    "        self.transform = timm.data.create_transform(**cfg)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def embed(self, batch_imgs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Given a batch of pre-transformed images, compute pooled embeddings.\n",
    "        The batch is moved to the proper device (with channels_last format) and processed in inference mode.\n",
    "        \"\"\"\n",
    "        batch_imgs = batch_imgs.to(self.device, non_blocking=True)\n",
    "        batch_imgs = batch_imgs.contiguous(memory_format=torch.channels_last)\n",
    "        if self.device.type == \"cuda\":\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                return self.model(batch_imgs)\n",
    "        else:\n",
    "            # autocast can be comically slow for some CPU setups (PyTorch issue #118499)\n",
    "            return self.model(batch_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@daft.udf(return_dtype=daft.DataType.list(daft.DataType.float32()))\n",
    "class EmbedImageCol:\n",
    "    \"\"\"run timm embedder on an image column\"\"\"\n",
    "    def __init__(self, model_name: str, batch_size: int = 4):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.embedder = Embedder(self.model_name)\n",
    "\n",
    "    def _pad_to_batch_size(batch, batch_size):\n",
    "        \"\"\"Pads the batch to batch_size with zeros\"\"\"\n",
    "        orig_size = len(batch)\n",
    "        batch = torch.stack(batch)\n",
    "        if orig_size < batch_size:\n",
    "            pad_tensor = torch.zeros((batch_size - orig_size, *batch.shape[1:]),\n",
    "                                    dtype=batch.dtype, device=batch.device)\n",
    "            batch = torch.cat([batch, pad_tensor], dim=0)\n",
    "        return batch, orig_size\n",
    "\n",
    "    def __call__(self, batch_images) -> torch.Tensor:\n",
    "        ### this needs to be lazy -- something like a dataloader\n",
    "        ### it's currently going to load the whole image_col\n",
    "        images = [self.embedder.transform(im) for im in batch_images.to_pylist()]\n",
    "        images = torch.cat(images, dim=0)\n",
    "        images, orig_size = self._pad_to_batch_size(images, self.batch_size)\n",
    "        embeddings = self.embedder.embed(images).cpu().numpy()\n",
    "        return embeddings[:orig_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/willsa/git/bedmap-dev/.venv/lib/python3.12/site-packages/daft/context.py:168: UserWarning: Daft is configured to use the new NativeRunner by default as of v0.4.0. If you are encountering any regressions, please switch back to the legacy PyRunner via `daft.context.set_runner_py()` or by setting the env variable `DAFT_RUNNER=py`. We appreciate you filing issues and helping make the NativeRunner better: https://github.com/Eventual-Inc/Daft/issues\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "glob = tmp_path.as_posix() +\"/*.png\"\n",
    "images_df = daft.from_glob_path(glob).with_column_renamed(\"path\", \"path_full_img\")\n",
    "\n",
    "images_df = images_df.with_column(\"image\", daft.col(\"path_full_img\"\n",
    "                                 ).url.download().image.decode(\n",
    "                                     mode=\"RGB\", on_error=\"null\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "EmbedImageColWithModel = EmbedImageCol.with_init_args(model_name=MODEL_NAME, batch_size=BATCH_SIZE\n",
    "                                                      ).override_options(batch_size=BATCH_SIZE)\n",
    "images_df = images_df.where(images_df[\"image\"].not_null()\n",
    "                            ).with_column(\"embed\", EmbedImageColWithModel(daft.col(\"image\"))\n",
    "                            ).exclude(\"image\", \"num_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Unoptimized Logical Plan ==\n",
      "\n",
      "* Project: col(path_full_img), col(size), col(embed)\n",
      "|\n",
      "* Project: col(path_full_img), col(size), col(num_rows), col(image),\n",
      "|     py_udf(col(image)) as embed\n",
      "|\n",
      "* Filter: not_null(col(image))\n",
      "|\n",
      "* Project: col(path_full_img), col(size), col(num_rows),\n",
      "|     image_decode(download(col(path_full_img))) as image\n",
      "|\n",
      "* Project: col(path) as path_full_img, col(size), col(num_rows)\n",
      "|\n",
      "* Source:\n",
      "|   Number of partitions = 1\n",
      "|   Output schema = path#Utf8, size#Int64, num_rows#Int64\n",
      "\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "\n",
      "* Project: col(path_full_img), col(size), py_udf(col(image)) as embed\n",
      "|   Stats = { Approx num rows = 1,800, Approx size bytes = 95.93 KiB, Accumulated\n",
      "|     selectivity = 0.90 }\n",
      "|\n",
      "* Filter: not_null(col(image))\n",
      "|   Stats = { Approx num rows = 1,800, Approx size bytes = 95.93 KiB, Accumulated\n",
      "|     selectivity = 0.90 }\n",
      "|\n",
      "* Project: col(path) as path_full_img, col(size), image_decode(download(col(path)\n",
      "|     as path_full_img)) as image\n",
      "|   Stats = { Approx num rows = 2,000, Approx size bytes = 106.59 KiB, Accumulated\n",
      "|     selectivity = 1.00 }\n",
      "|\n",
      "* Source:\n",
      "|   Number of partitions = 1\n",
      "|   Output schema = path#Utf8, size#Int64, num_rows#Int64\n",
      "|   Stats = { Approx num rows = 2,000, Approx size bytes = 106.59 KiB, Accumulated\n",
      "|     selectivity = 1.00 }\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "\n",
      "* Project: col(path_full_img), col(size), py_udf(col(image)) as embed\n",
      "|   Resource request = {  }\n",
      "|   Stats = { Approx num rows = 1,800, Approx size bytes = 95.93 KiB, Accumulated\n",
      "|     selectivity = 0.90 }\n",
      "|\n",
      "* Filter: not_null(col(image))\n",
      "|   Stats = { Approx num rows = 1,800, Approx size bytes = 95.93 KiB, Accumulated\n",
      "|     selectivity = 0.90 }\n",
      "|\n",
      "* Project: col(path) as path_full_img, col(size), image_decode(download(col(path)\n",
      "|     as path_full_img)) as image\n",
      "|   Resource request = None\n",
      "|   Stats = { Approx num rows = 2,000, Approx size bytes = 106.59 KiB, Accumulated\n",
      "|     selectivity = 1.00 }\n",
      "|\n",
      "* InMemorySource:\n",
      "|   Schema = path#Utf8, size#Int64, num_rows#Int64\n",
      "|   Size bytes = 109148\n",
      "|   Stats = { Approx num rows = 2,000, Approx size bytes = 106.59 KiB, Accumulated\n",
      "|     selectivity = 1.00 }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images_df.explain(show_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0df516039224563acc6f59bdce553d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üó°Ô∏è üêü Filter: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da54ad152a24775a9fc080ded7db462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üó°Ô∏è üêü Project: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c71100f4384792b8f5c08c87a5593e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üó°Ô∏è üêü Project: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "images_df = images_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "shutil.rmtree(tmp_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
