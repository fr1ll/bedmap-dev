{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp daft_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/willsa/git/bedmap-dev/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "import daft\n",
    "import timm\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "daft.set_execution_config(enable_native_executor=True,\n",
    "                          default_morsel_size=1\n",
    "                          )\n",
    "\n",
    "MAX_IMAGES = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:40, 49.82it/s] \n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "tmp_path = Path(\"./tmp-test-images\")\n",
    "tmp_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dataset_name = \"kvriza8/microscopy_images\"\n",
    "\n",
    "dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "\n",
    "for i, example in enumerate(tqdm(iter(dataset))):  # Use iterator to avoid full load\n",
    "    if i >= MAX_IMAGES:\n",
    "        break\n",
    "    image = example[\"image\"]\n",
    "    image.save(tmp_path / f\"{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dataclass\n",
    "class TimmEmbedder:\n",
    "    \"\"\"\n",
    "    embed an image with any timm model that supports this\n",
    "\n",
    "    Reference: https://huggingface.co/docs/timm/main/en/feature_extraction#pooled\n",
    "    \"\"\"\n",
    "    model_name: str\n",
    "    device: torch.device = field(init=False)\n",
    "    dtype: torch.dtype = field(init=False)\n",
    "    _model: Callable = field(init=False)\n",
    "    _transforms: Callable = field(init=False)\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # initialize model and transforms\n",
    "        self._model = timm.create_model(self.model_name, pretrained=True, num_classes=0)\n",
    "        cfg = self._model.pretrained_cfg\n",
    "        self._transform = timm.data.create_transform(**timm.data.resolve_data_config(cfg))\n",
    "\n",
    "        # set device and dtype\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.dtype = torch.bfloat16 if self.device.type == \"cuda\" and torch.cuda.is_bf16_supported() else (\n",
    "            torch.float16 if self.device.type == \"cuda\" else torch.float32\n",
    "        )\n",
    "        print(f\"Inference device: {self.device} with dtype: {self.dtype}\")\n",
    "\n",
    "        # optimize model for inference\n",
    "        self._model = self._model.eval().to(device=self.device, dtype=self.dtype)\n",
    "        self._model = torch.jit.optimize_for_inference(torch.jit.script(self._model))\n",
    "\n",
    "\n",
    "    def __call__(self, image: torch.Tensor | Image.Image) -> np.array:\n",
    "            \"\"\"transform image, run inference, extract embedding as 1D array\"\"\"\n",
    "            image = self._transform(image).to(self.device, self.dtype).unsqueeze(0)\n",
    "            emb = self._model(image)\n",
    "            return emb.detach().cpu().float().numpy().squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# it's a one-stop shop\n",
    "# e = TimmEmbedder(\"mobilenetv3_large_100\")\n",
    "# e(torch.rand((3,256,256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@daft.udf(return_dtype=daft.DataType.list(daft.DataType.float32()), batch_size=1)\n",
    "class EmbedImages:\n",
    "    \"\"\"run timm embedder on an image column\"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.embedder = TimmEmbedder(self.model_name)\n",
    "\n",
    "    def __call__(self, images_col):\n",
    "        images = [rearrange(im, \"h w c -> c h w\") for im in images_col.to_pylist()]\n",
    "        ## Note: expect images are different sizes\n",
    "        ## could maybe speed up by doing the resize transform separately,\n",
    "        ## then doing batch inference\n",
    "        ## Example: https://colab.research.google.com/github/Eventual-Inc/Daft/blob/main/tutorials/mnist.ipynb\n",
    "        return [self.embedder(\n",
    "            torch.tensor(im, dtype=self.embedder.dtype)) for im in images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/willsa/git/bedmap-dev/.venv/lib/python3.12/site-packages/daft/context.py:168: UserWarning: Daft is configured to use the new NativeRunner by default as of v0.4.0. If you are encountering any regressions, please switch back to the legacy PyRunner via `daft.context.set_runner_py()` or by setting the env variable `DAFT_RUNNER=py`. We appreciate you filing issues and helping make the NativeRunner better: https://github.com/Eventual-Inc/Daft/issues\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "glob = tmp_path.as_posix() +\"/*.png\"\n",
    "images_df = daft.from_glob_path(glob).with_column_renamed(\"path\", \"path_full_img\")\n",
    "\n",
    "images_df = images_df.with_column(\"image\", daft.col(\"path_full_img\"\n",
    "                                 ).url.download().image.decode(\n",
    "                                     mode=\"RGB\", on_error=\"null\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference time for some embedding models on CPU on my laptop\n",
    "\n",
    "model_name | 50 images | 500 images | 1000 images | 2000 images\n",
    "---------- | --------- | ---------- | ----------- | -----------\n",
    "mobilenetv3_large_100 | 0m02s |  0m8s | 0m12s | 0m25s\n",
    "vit_small_patch14_reg4_dinov2.lvd142m | 0m26s | 4m26s | 8m52s | 17m17s \n",
    "vit_base_patch14_reg4_dinov2.lvd142m | 1m20s | 11m48s | 23m29s | OOM\n",
    "vit_large_patch14_reg4_dinov2.lvd142m | 4m04s | 38m30s | OOM | OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "MODEL_NAME = \"vit_base_patch14_reg4_dinov2.lvd142m\"\n",
    "EmbedImagesWithModel = EmbedImages.with_init_args(MODEL_NAME)\n",
    "\n",
    "images_df = images_df.with_column(\"embed\", EmbedImagesWithModel(daft.col(\"image\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "images_df = images_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import shutil\n",
    "shutil.rmtree(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `timm/mobilenetv3_large_100.ra_in1k` from `timm`...\n",
      "┌──────────────────────────────────────────────────────────────────┐\n",
      "│  Memory Usage for loading `timm/mobilenetv3_large_100.ra_in1k`   │\n",
      "├───────┬─────────────┬──────────┬─────────────────────────────────┤\n",
      "│ dtype │Largest Layer│Total Size│       Training using Adam       │\n",
      "├───────┼─────────────┼──────────┼─────────────────────────────────┤\n",
      "│float32│   4.89 MB   │ 21.01 MB │             84.04 MB            │\n",
      "└───────┴─────────────┴──────────┴─────────────────────────────────┘\n",
      "\n",
      "Loading pretrained config for `timm/vit_small_patch14_reg4_dinov2.lvd142m` from `timm`...\n",
      "┌──────────────────────────────────────────────────────────────────────────────────┐\n",
      "│      Memory Usage for loading `timm/vit_small_patch14_reg4_dinov2.lvd142m`       │\n",
      "├───────┬─────────────┬──────────┬─────────────────────────────────────────────────┤\n",
      "│ dtype │Largest Layer│Total Size│               Training using Adam               │\n",
      "├───────┼─────────────┼──────────┼─────────────────────────────────────────────────┤\n",
      "│float32│   2.26 MB   │ 84.14 MB │                    336.57 MB                    │\n",
      "└───────┴─────────────┴──────────┴─────────────────────────────────────────────────┘\n",
      "\n",
      "Loading pretrained config for `timm/vit_base_patch14_reg4_dinov2.lvd142m` from `timm`...\n",
      "┌────────────────────────────────────────────────────────────────────────────────┐\n",
      "│      Memory Usage for loading `timm/vit_base_patch14_reg4_dinov2.lvd142m`      │\n",
      "├───────┬─────────────┬──────────┬───────────────────────────────────────────────┤\n",
      "│ dtype │Largest Layer│Total Size│              Training using Adam              │\n",
      "├───────┼─────────────┼──────────┼───────────────────────────────────────────────┤\n",
      "│float32│   9.01 MB   │330.28 MB │                    1.29 GB                    │\n",
      "└───────┴─────────────┴──────────┴───────────────────────────────────────────────┘\n",
      "\n",
      "Loading pretrained config for `timm/vit_large_patch14_reg4_dinov2.lvd142m` from `timm`...\n",
      "┌──────────────────────────────────────────────────────────────────────────────────┐\n",
      "│      Memory Usage for loading `timm/vit_large_patch14_reg4_dinov2.lvd142m`       │\n",
      "├───────┬─────────────┬──────────┬─────────────────────────────────────────────────┤\n",
      "│ dtype │Largest Layer│Total Size│               Training using Adam               │\n",
      "├───────┼─────────────┼──────────┼─────────────────────────────────────────────────┤\n",
      "│float32│   16.02 MB  │ 1.13 GB  │                     4.54 GB                     │\n",
      "└───────┴─────────────┴──────────┴─────────────────────────────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [\"mobilenetv3_large_100.ra_in1k\",\n",
    "          \"vit_small_patch14_reg4_dinov2.lvd142m\",\n",
    "          \"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "          \"vit_large_patch14_reg4_dinov2.lvd142m\"]\n",
    "\n",
    "from subprocess import run\n",
    "\n",
    "results = []\n",
    "for m in models:\n",
    "    cmd = [\"accelerate\", \"estimate-memory\", f\"timm/{m}\", \"--dtypes\", \"float32\"]\n",
    "    result = run(cmd, capture_output=True)\n",
    "    results += [result.stdout]\n",
    "\n",
    "for r in results:\n",
    "    print(r.decode())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
