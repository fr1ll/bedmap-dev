# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/011_prepare-images.ipynb.

# %% auto 0
__all__ = ['cfg', 'BATCH_SIZE', 'MODEL_NAME', 'dl_hf_images']

# %% ../../nbs/011_prepare-images.ipynb 4
from pathlib import Path
import shutil
import sys

import daft
from humanize import naturalsize
from tqdm import tqdm
from loguru import logger

from .validate_images import validate_images
from .create_thumbnails import create_thumbnails
from .embed_images import embed_images
from .config import ImageConfig

from fastcore.test import test_eq

# %% ../../nbs/011_prepare-images.ipynb 5
# easy timestamps

logger.remove()
logger.add(sys.stdout, level="INFO")

# %% ../../nbs/011_prepare-images.ipynb 6
cfg = ImageConfig()

BATCH_SIZE = 4
MODEL_NAME = cfg.model_name

# %% ../../nbs/011_prepare-images.ipynb 8
def _image_glob_pattern(directory: str | Path) -> str:
    """Convert a directory path into a glob pattern that matches common image formats.
    
    Args:
        directory: Directory path to search for images
        
    Returns:
        Glob pattern string that matches common image formats
    """
    exts = ['jpg', 'jpeg', 'png', 'gif', 'bmp', 'webp', 'tiff', 'tif']
    dir_str = str(directory).rstrip('/')
    return f"{dir_str}/**/*.{{{','.join(exts)}}}"
    

# %% ../../nbs/011_prepare-images.ipynb 10
def _df_images_from_pattern(pattern: str | Path) -> daft.DataFrame:
    """Create a dataframe from a glob pattern of images.
    
    Args:
        pattern: Directory or glob pattern to match image files
        
    Returns:
        DataFrame with columns:
        - path: Path to the file
        - size: Size in bytes
    """
    pattern = str(pattern)
    if '*' not in pattern: pattern = _image_glob_pattern(pattern)
    df_img = daft.from_glob_path(pattern).with_column_renamed("path", "img_path")
    df_img = df_img.with_column("img_name", df_img["img_path"].str.split("/").list.get(-1).cast(str))
    df_img = df_img.with_column(
        "img", daft.col("img_path").url.download(on_error="null").image.decode(on_error="null", mode="RGB")
    ).exclude("num_rows")
    return df_img

# %% ../../nbs/011_prepare-images.ipynb 12
def dl_hf_images(dataset_name: str = "kvriza8/microscopy_images",
                 dir: Path = None,
                 max_images: int = 64,
                 overwrite: bool = True,
                 format: str = "png") -> None:
    """Download images from a Hugging Face dataset.
    
    Args:
        dataset_name: Name of the Hugging Face dataset to download from
        dir: Directory to save images to. If None, creates a directory named after the dataset
        max_images: Maximum number of images to download
        overwrite: Whether to overwrite existing directory if it exists
        format: Image format to save as (png, jpg, etc)
        
    Returns:
        None
    """
    from datasets import load_dataset

    dataset = load_dataset(dataset_name, split="train", streaming=True)
    if overwrite:
        shutil.rmtree(dir, ignore_errors=True)
        dir.mkdir(parents=True, exist_ok=True)

    image_paths = []
    for i, img_row in enumerate(tqdm(iter(dataset), total=max_images)):
        if i >= max_images:
            break
        img = img_row["image"]
        image_paths += [(dir / f"{i}.{format}")]
        img.save(image_paths[-1])

    print(f"Size of images on disk: {naturalsize(sum([p.stat().st_size for p in image_paths]))}")

    return None

# %% ../../nbs/011_prepare-images.ipynb 18
def _embed_images_for_df(df: daft.DataFrame) -> daft.DataFrame:
    """
    Embed images for a given dataframe.
    """
    ## Surely there's a cleaner way to get the paths out

    paths = [Path(i["img_path"].lstrip("file:/")) for i in df.select("img_path").to_pylist()]
    embeds = embed_images(paths, model_name=MODEL_NAME, batch_size=BATCH_SIZE)
    embeds_type = daft.DataType.embedding(daft.DataType.float32(), embeds.shape[-1])
    embeds_series = daft.Series.from_numpy(embeds).cast(embeds_type)

    return images_df.with_column("embeds", daft.lit(embeds_series))
