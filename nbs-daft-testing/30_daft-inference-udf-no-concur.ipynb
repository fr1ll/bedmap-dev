{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# try not to run concurrent inference processes\n",
    "# instead do batch inference with correct batch size\n",
    "os.environ[\"DAFT_ENABLE_ACTOR_POOL_PROJECTIONS\"]=\"1\"\n",
    "import gc\n",
    "import sys\n",
    "import shutil\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import daft\n",
    "import numpy as np\n",
    "import einops\n",
    "import timm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from humanize import naturalsize\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# easy timestamps\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use daft for the end-to-end\n",
    "\n",
    "Result: Peak memory usage drops from 3.0 Gb to 2.1 Gb. This is still more than plain torch at 1.9 Gb, but it's much much better\n",
    "\n",
    "## TODO:\n",
    "- [x] fix NoneType issue\n",
    "    - fixed by... returning from `__call__`\n",
    "- [ ] test using url.decode here\n",
    "- [x] remove DAFT_ENABLE_ACTOR_POOL_PROJECTIONS=1\n",
    "    - result: max 2.248 Gb\n",
    "- [x] test setting morsel size to batch size and native executor = True\n",
    "    - result: initializes 8 parallel UDFs. Max memory 3.7 Gb\n",
    "- [x] test skipping embedder UDF\n",
    "    - result: 1. Gb max memory, no NoneType error, 8 parallel UDF instances\n",
    "- [x] test manually setting concurrency level to 1\n",
    "    - result: `invalid record subtype` error\n",
    "- [x] test native runner, fixed NoneType error, enable pool projections 1, morsel size is batch size\n",
    "    - result tmpatnsi3en: ran 8.5 minutes, spikey memory graph, dropped over time, max 4 Gb\n",
    "- [x] test embedder UDF has num_cpus=16 to see if that will avoid concurrency with native runner\n",
    "    - result: waited between loading first embedder UDF and others..max mem 2.36 Gb, took 4 minutes to finish\n",
    "- [x] test both UDFs have num_cpus=16\n",
    "    - result: similar to previous one in log messages; max mem 2.4 Gb, took 3.5 min to finish\n",
    "- [x] test doing load pillow within the embedding UDF to avoid pileup of images beforehand\n",
    "    - result: max resident size 4 Gb, but diverges from max heap size <1Gb. Runs in 3m10s, but actual embedding pipeline 2min10s.\n",
    "- [x] try same as above with native runner \"false\"\n",
    "    - result: resident size 2.3 Gb, heap size 1.1 Gb, took about 2m16s for embedding pipeline itself\n",
    "- [ ] try with native and remove morsel size parameter\n",
    "- [ ] test (separate notebook) replicating the transform function outside the embedder class as a UDF to see if that's what causes memory duplication in notebook 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set variables for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DaftContext(_ctx=<builtins.PyDaftContext object at 0x7fdd18877990>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "MODEL_NAME = \"vit_small_patch14_reg4_dinov2.lvd142m\"\n",
    "TEST_DATASET = \"kvriza8/microscopy_images\"\n",
    "NUM_TEST_IMAGES = 256\n",
    "\n",
    "# daft.set_execution_config(enable_native_executor=True)\n",
    "daft.set_execution_config(default_morsel_size=BATCH_SIZE, enable_native_executor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define way to download small test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_hf_images(dataset_name: str = \"kvriza8/microscopy_images\",\n",
    "                 dir: Path = None,\n",
    "                 max_images: int = 64,\n",
    "                 overwrite: bool = True,\n",
    "                 format: str = \"png\") -> None:\n",
    "\n",
    "    dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "    if overwrite:\n",
    "        shutil.rmtree(dir, ignore_errors=True)\n",
    "        dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    image_paths = []\n",
    "    for i, img_row in enumerate(tqdm(iter(dataset), total=max_images)):\n",
    "        if i >= max_images:\n",
    "            break\n",
    "        img = img_row[\"image\"]\n",
    "        image_paths += [(dir / f\"{i}.{format}\")]\n",
    "        img.save(image_paths[-1])\n",
    "\n",
    "    logger.info(f\"Size of images on disk: {naturalsize(sum([p.stat().st_size for p in image_paths]))}\")\n",
    "\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define timm-based embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embedder:\n",
    "    \"\"\"instantiate pretrained timm model to generate embeddings\"\"\"\n",
    "    def __init__(self, model_name: str, device: torch.device = None):\n",
    "        self.model_name = model_name\n",
    "        # choose device and dtype\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if self.device.type == \"cuda\":\n",
    "            self.dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "        else:\n",
    "            self.dtype = torch.float32\n",
    "\n",
    "        # Create and prepare the model\n",
    "        self.model = timm.create_model(self.model_name, pretrained=True, num_classes=0)\n",
    "        self.model.to(self.device, memory_format=torch.channels_last)\n",
    "        self.model.eval()\n",
    "        self.model = torch.compile(self.model, dynamic=True, mode=\"reduce-overhead\")\n",
    "\n",
    "        # must resolve config to drop unneeded fields\n",
    "        cfg = timm.data.resolve_data_config(self.model.pretrained_cfg)\n",
    "        self.transform = timm.data.create_transform(**cfg)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def embed(self, batch_imgs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"set up input and embed it\"\"\"\n",
    "        batch_imgs = batch_imgs.to(self.device, non_blocking=True, memory_format=torch.channels_last)\n",
    "        if self.device.type == \"cuda\":\n",
    "            with torch.amp.autocast(\"cuda\", dtype=self.dtype):\n",
    "                return self.model(batch_imgs)\n",
    "        return self.model(batch_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define two types of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@daft.udf(return_dtype=daft.DataType.python(), batch_size=BATCH_SIZE, num_cpus=16)\n",
    "class LoadPillow:\n",
    "    \"\"\"run path column as PIL Image\"\"\"\n",
    "    def __init__(self):\n",
    "        self._pil_load = lambda x: Image.open(x).convert(\"RGB\")\n",
    "        logger.info(\"Initializing Pillow loader UDF\")\n",
    "\n",
    "    def __call__(self, batch_paths: daft.Series) -> list:\n",
    "        batch_paths = [p.replace(\"file://\",\"\") for p in batch_paths.to_pylist()]\n",
    "        return [self._pil_load(p) for p in batch_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@daft.udf(return_dtype=daft.DataType.python(), batch_size=BATCH_SIZE, num_cpus=16)\n",
    "class DaftTimmEmbed:\n",
    "    \"\"\"transform and embed images with timm pre-trained model\"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        self._embedder = Embedder(model_name)\n",
    "        logger.info(f\"Initialize embedder on device {self._embedder.device}\")\n",
    "        logger.info(f\"with dtype {self._embedder.dtype}\")\n",
    "\n",
    "\n",
    "    def __call__(self, batch_images: daft.Series) -> list:\n",
    "        batch_t = [self._embedder.transform(im) for im in batch_images.to_pylist()]\n",
    "        batch_t = torch.stack(batch_t).to(memory_format=torch.channels_last)\n",
    "        return list(self._embedder.embed(batch_t).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@daft.udf(return_dtype=daft.DataType.python(), batch_size=BATCH_SIZE, num_cpus=16)\n",
    "class DaftTimmEmbedFromPath:\n",
    "    \"\"\"transform and embed images with timm pre-trained model\"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        self._embedder = Embedder(model_name)\n",
    "        self._pil_load = lambda x: Image.open(x).convert(\"RGB\")\n",
    "        logger.info(f\"Initialize embedder on device {self._embedder.device}\")\n",
    "        logger.info(f\"with dtype {self._embedder.dtype}\")\n",
    "\n",
    "\n",
    "    def __call__(self, batch_paths: daft.Series) -> list:\n",
    "        batch_paths = [p.replace(\"file://\",\"\") for p in batch_paths.to_pylist()]\n",
    "        batch_t = [self._embedder.transform(self._pil_load(p)) for p in batch_paths]\n",
    "        batch_t = torch.stack(batch_t).to(memory_format=torch.channels_last)\n",
    "        return list(self._embedder.embed(batch_t).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedding computation pipeline including dataset instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daft_glob_infer(image_glob: str,\n",
    "                    batch_size: int = 32):\n",
    "    \"\"\"generate a torch image dataset via daft from a glob\"\"\"\n",
    "\n",
    "    images_df = daft.from_glob_path(image_glob)\n",
    "    DaftTimmEmbedwModel = DaftTimmEmbed.with_init_args(model_name=MODEL_NAME)\n",
    "\n",
    "    images_df = images_df.with_column(\"image\", LoadPillow(daft.col(\"path\"))\n",
    "                                    ).exclude(\"num_rows\")\n",
    "    images_df = images_df.where(images_df[\"image\"].not_null())\n",
    "    images_df = images_df.with_column(\"embedding\",\n",
    "                                      DaftTimmEmbedwModel(daft.col(\"image\"))\n",
    "                                      ).exclude(\"image\")\n",
    "    return images_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daft_glob_infer_oneudf(image_glob: str,\n",
    "                    batch_size: int = 32):\n",
    "    \"\"\"generate a torch image dataset via daft from a glob\"\"\"\n",
    "\n",
    "    images_df = daft.from_glob_path(image_glob)\n",
    "    DaftTimmFromPathwModel = DaftTimmEmbedFromPath.with_init_args(model_name=MODEL_NAME)\n",
    "    images_df = images_df.with_column(\"embedding\",\n",
    "                                      DaftTimmFromPathwModel(daft.col(\"path\"))\n",
    "                                      )\n",
    "    return images_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Do memory profiling with one type of dataset\n",
    "\n",
    "Results written near top of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-25 23:10:10.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading test images.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memray WARNING: Correcting symbol for malloc from 0x420620 to 0x7fdd9921ec60\n",
      "Memray WARNING: Correcting symbol for free from 0x420ab0 to 0x7fdd9921f370\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 256/256 [00:09<00:00, 26.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-25 23:10:22.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdl_hf_images\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mSize of images on disk: 19.5 MB\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-25 23:10:22.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mStarting embedding pipeline.\u001b[0m\n",
      "\u001b[32m2025-03-25 23:10:22.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mSet up embedding dataframe\u001b[0m\n",
      "\u001b[32m2025-03-25 23:10:25.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mInitialize embedder on device cpu\u001b[0m\n",
      "\u001b[32m2025-03-25 23:10:25.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mwith dtype torch.float32\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9155df59fd1847bc85953ccb4ba3af80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üó°Ô∏è üêü Project: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-25 23:11:05.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mInitialize embedder on device cpu\u001b[0m\n",
      "\u001b[32m2025-03-25 23:11:05.820\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mwith dtype torch.float32\u001b[0m\n",
      "\u001b[32m2025-03-25 23:11:18.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mInitialize embedder on device cpu\u001b[0m\n",
      "\u001b[32m2025-03-25 23:11:18.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mwith dtype torch.float32\u001b[0m\n",
      "\u001b[32m2025-03-25 23:11:31.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mInitialize embedder on device cpu\u001b[0m\n",
      "\u001b[32m2025-03-25 23:11:31.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mwith dtype torch.float32\u001b[0m\n",
      "\u001b[32m2025-03-25 23:11:44.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mInitialize embedder on device cpu\u001b[0m\n",
      "\u001b[32m2025-03-25 23:11:44.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mwith dtype torch.float32\u001b[0m\n",
      "\u001b[32m2025-03-25 23:11:57.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mInitialize embedder on device cpu\u001b[0m\n",
      "\u001b[32m2025-03-25 23:11:57.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mwith dtype torch.float32\u001b[0m\n",
      "\u001b[32m2025-03-25 23:12:10.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mInitialize embedder on device cpu\u001b[0m\n",
      "\u001b[32m2025-03-25 23:12:10.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mwith dtype torch.float32\u001b[0m\n",
      "\u001b[32m2025-03-25 23:12:23.431\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mInitialize embedder on device cpu\u001b[0m\n",
      "\u001b[32m2025-03-25 23:12:23.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mwith dtype torch.float32\u001b[0m\n",
      "\u001b[32m2025-03-25 23:12:35.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mDone with embedding pipeline.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e686bc0ccf4456bd8bdd2d95dad7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ö† <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> No debug information was found for the Python interpreter </span> ‚ö†\n",
       "\n",
       "Without debug information reports showing native traces <span style=\"font-weight: bold\">may not include file names and line numbers</span>. Please use an \n",
       "interpreter built with debug symbols for best results. Check <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://bloomberg.github.io/memray/native_mode.html</span> \n",
       "for more information regarding how memray resolves symbols.\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ö† \u001b[1;33m No debug information was found for the Python interpreter \u001b[0m ‚ö†\n",
       "\n",
       "Without debug information reports showing native traces \u001b[1mmay not include file names and line numbers\u001b[0m. Please use an \n",
       "interpreter built with debug symbols for best results. Check \u001b[4;94mhttps://bloomberg.github.io/memray/native_mode.html\u001b[0m \n",
       "for more information regarding how memray resolves symbols.\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207f55252a2545ec8e8d3efdf58db42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Results saved to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">/home/willsa/git/bedmap-dev/nbs/daft-try/memray-results/tmpjgxydte5/flamegraph.html</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Results saved to \u001b[1;36m/home/willsa/git/bedmap-dev/nbs/daft-try/memray-results/tmpjgxydte5/\u001b[0m\u001b[1;36mflamegraph.html\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600\"\n",
       "            src=\"/home/willsa/git/bedmap-dev/nbs/daft-try/memray-results/tmpjgxydte5/flamegraph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fdcc41dccb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%memray_flamegraph --native --follow-fork --temporal\n",
    "\n",
    "with TemporaryDirectory() as tmp:\n",
    "    logger.info(\"Downloading test images.\")\n",
    "    dl_hf_images(dir=Path(tmp), max_images=NUM_TEST_IMAGES)\n",
    "    imglob = tmp+\"/*.png\"\n",
    "    logger.info(\"Starting embedding pipeline.\")\n",
    "    df_embeds = daft_glob_infer_oneudf(imglob)\n",
    "    logger.info(\"Set up embedding dataframe\")\n",
    "    df_embeds = df_embeds.collect()\n",
    "    logger.info(\"Done with embedding pipeline.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
