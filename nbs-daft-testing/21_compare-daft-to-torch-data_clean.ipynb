{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import shutil\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import daft\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from humanize import naturalsize\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# easy timestamps\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare daft dataframe -> torch dataset vs. plain torch dataset\n",
    "\n",
    "I am looking at using daft to do image processing in `bedmap` (F.N.A. clipplot forked from PixPlot)\n",
    "\n",
    "This library runs a timm embedding model against a folder of images on disk.\n",
    "\n",
    "It could later be extended to images on object store or other sources.\n",
    "Daft simplifies some of the initial cleanup pipeline, so it would be nice to \n",
    "\n",
    "Steps in this notebook:\n",
    "1. Set variables for test\n",
    "2. Define function to download some test images\n",
    "3. Define \"embedder\" which is a timm pre-trained image model instantiated to do embeddings\n",
    "4. Define two types of datasets (daft_to_torch and plain_torch)\n",
    "5. Define embedding pipeline including the dataset creation\n",
    "6. Do memory profiling with one dataset option\n",
    "\n",
    "NOTE: This notebook must be run twice to create comparative memory profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results summary from profiling last cell\n",
    "\n",
    "Constants for test:\n",
    "256 images, batch_size 32, model \"vit_small_patch14_reg4_dinov2.lvd142m\"\n",
    "\n",
    "### daft_to_torch:\n",
    "- Peak resident size: 3.24 Gb\n",
    "- Peak heap size: 2.46 Gb\n",
    "- Resident size just before first inference: 2.42 Gb\n",
    "- Images per second: 2.0\n",
    "- Profile graphs: ./memray-results-summary/2025-03-14_daft-to-torch.html\n",
    "\n",
    "### plain_torch:\n",
    "- Peak resident size: 1.89 Gb\n",
    "- Peak heap size: 1.15 Gb\n",
    "- Resident size just before first inference: 1.08 Gb\n",
    "- Images per second: 1.95\n",
    "- Profile graphs: ./memray-results-summary/2025-03-14_plain-torch.html\n",
    "\n",
    "### Thoughts\n",
    "Memory difference seems all due to something during initialization\n",
    "at first inference call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set variables for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DAFT: bool = True # else use torch dataset\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MODEL_NAME = \"vit_small_patch14_reg4_dinov2.lvd142m\"\n",
    "TEST_DATASET = \"kvriza8/microscopy_images\"\n",
    "NUM_TEST_IMAGES = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define way to download small test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_hf_images(dataset_name: str = \"kvriza8/microscopy_images\",\n",
    "                 dir: Path = None,\n",
    "                 max_images: int = 64,\n",
    "                 overwrite: bool = True,\n",
    "                 format: str = \"png\") -> None:\n",
    "\n",
    "    dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "    if overwrite:\n",
    "        shutil.rmtree(dir, ignore_errors=True)\n",
    "        dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    image_paths = []\n",
    "    for i, img_row in enumerate(tqdm(iter(dataset), total=max_images)):\n",
    "        if i >= max_images:\n",
    "            break\n",
    "        img = img_row[\"image\"]\n",
    "        image_paths += [(dir / f\"{i}.{format}\")]\n",
    "        img.save(image_paths[-1])\n",
    "\n",
    "    print(f\"Size of images on disk: {naturalsize(sum([p.stat().st_size for p in image_paths]))}\")\n",
    "\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define timm-based embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embedder:\n",
    "    \"\"\"instantiate pretrained timm model to generate embeddings\"\"\"\n",
    "    def __init__(self, model_name: str, device: torch.device = None):\n",
    "        self.model_name = model_name\n",
    "        # choose device and dtype\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if self.device.type == \"cuda\":\n",
    "            self.dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "        else:\n",
    "            self.dtype = torch.float32\n",
    "\n",
    "        # Create and prepare the model\n",
    "        self.model = timm.create_model(self.model_name, pretrained=True, num_classes=0)\n",
    "        self.model.to(self.device, memory_format=torch.channels_last)\n",
    "        self.model.eval()\n",
    "        self.model = torch.compile(self.model, dynamic=True, mode=\"reduce-overhead\")\n",
    "\n",
    "        # must resolve config to drop unneeded fields\n",
    "        cfg = timm.data.resolve_data_config(self.model.pretrained_cfg)\n",
    "        self.transform = timm.data.create_transform(**cfg)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def embed(self, batch_imgs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"set up input and embed it\"\"\"\n",
    "        batch_imgs = batch_imgs.to(self.device, non_blocking=True, memory_format=torch.channels_last)\n",
    "        if self.device.type == \"cuda\":\n",
    "            with torch.amp.autocast(\"cuda\", dtype=self.dtype):\n",
    "                return self.model(batch_imgs)\n",
    "        return self.model(batch_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define two types of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@daft.udf(return_dtype=daft.DataType.python())\n",
    "class TransformImagesDaft:\n",
    "    \"\"\"run timm embedder on an image column\"\"\"\n",
    "    def __init__(self, transform: callable):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, batch_images) -> list:\n",
    "        return [self.transform(Image.fromarray(im)) for im in batch_images.to_pylist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TorchImageIterAsDict(Dataset):\n",
    "    def __init__(self, filelist: list[Path], transform: callable):\n",
    "        self.filelist = filelist\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filelist)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image = Image.open(self.filelist[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            return {\"image_transformed\": self.transform(image)}\n",
    "        # return as dict for easy comparison vs. daft\n",
    "        else:\n",
    "            return {\"image\": [image]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daft_to_torch_iter_from_glob(image_glob: str, transform: callable):\n",
    "    \"\"\"generate a torch image dataset via daft from a glob\"\"\"\n",
    "\n",
    "    images_df = daft.from_glob_path(image_glob)\n",
    "    images_df = images_df.with_column(\"image\", daft.col(\"path\"\n",
    "                                    ).url.download().image.decode(\n",
    "                                        mode=\"RGB\", on_error=\"null\")\n",
    "                                    )\n",
    "    images_df = images_df.where(images_df[\"image\"].not_null())\n",
    "    TransformImForModel = TransformImagesDaft.with_init_args(transform=transform)\n",
    "    images_df = images_df.with_column(\"image_transformed\", TransformImForModel(daft.col(\"image\"))\n",
    "                                    ).exclude(\"image\", \"num_rows\")\n",
    "    return images_df.to_torch_iter_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_iter_from_glob(image_glob: str, transform: callable):\n",
    "    \"\"\"generate a torch image dataset via daft from a glob\"\"\"\n",
    "\n",
    "    image_list = [Path(p) for p in glob(image_glob)]\n",
    "    return TorchImageIterAsDict(image_list, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedding computation pipeline including dataset instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(model_name: str, images_glob: str, batch_size: int = BATCH_SIZE,\n",
    "                       dataset_type: str = \"plain_torch\"\n",
    "                       ) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Return a list of embeddings from a glob of images.\n",
    "    Uses a timm pretrained model to generate embeddings\n",
    "    \"\"\"\n",
    "    logger.info(\"Instantiating embedding model.\")\n",
    "    embedder = Embedder(model_name=model_name)\n",
    "\n",
    "    logger.info(f\"Creating dataset of type {dataset_type}.\")\n",
    "    if dataset_type == \"daft_to_torch\":\n",
    "        dataset = daft_to_torch_iter_from_glob(images_glob, embedder.transform)\n",
    "    elif dataset_type == \"plain_torch\":\n",
    "        dataset = torch_iter_from_glob(images_glob, embedder.transform)\n",
    "    else:\n",
    "        raise ValueError(\"Dataset type must be `daft_to_torch` or `plain_torch`.\")\n",
    "\n",
    "    logger.info(\"Creating dataloader.\")\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    logger.info(\"Generating embeddings.\")\n",
    "    for i, batch_images in enumerate(tqdm(dataloader, unit_scale=BATCH_SIZE)):\n",
    "        emb = embedder.embed(batch_images[\"image_transformed\"]).cpu().numpy()\n",
    "        # if i == 0:\n",
    "        #     print(f\"Shape of embedding for one batch: {emb.shape}\")\n",
    "        embeddings.append(emb)\n",
    "    logger.info(\"Stacking embeddings.\")\n",
    "    embeddings = np.vstack(embeddings)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Do memory profiling with one type of dataset\n",
    "\n",
    "Results written near top of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-14 15:08:48.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mDownloading test images.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memray WARNING: Correcting symbol for malloc from 0x420620 to 0x7fceea689c60\n",
      "Memray WARNING: Correcting symbol for free from 0x420ab0 to 0x7fceea68a370\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 256/256 [00:10<00:00, 24.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of images on disk: 19.5 MB\n",
      "\u001b[32m2025-03-14 15:09:03.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mStarting embedding pipeline.\u001b[0m\n",
      "\u001b[32m2025-03-14 15:09:03.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompute_embeddings\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mInstantiating embedding model.\u001b[0m\n",
      "\u001b[32m2025-03-14 15:09:05.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompute_embeddings\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mCreating dataset of type daft_to_torch.\u001b[0m\n",
      "\u001b[32m2025-03-14 15:09:05.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompute_embeddings\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mCreating dataloader.\u001b[0m\n",
      "\u001b[32m2025-03-14 15:09:05.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompute_embeddings\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mGenerating embeddings.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fe49e7918e4ee697c77826282c9194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üó°Ô∏è üêü Project: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9047a96455416999a0c18d3a4171f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üó°Ô∏è üêü Filter: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d16bd8990e4ce29cb0df166df92086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üó°Ô∏è üêü Project: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [02:06,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-14 15:11:12.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompute_embeddings\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mStacking embeddings.\u001b[0m\n",
      "\u001b[32m2025-03-14 15:11:12.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mDone with embedding pipeline.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c198b5fbabc4a4995b7720db9cf3b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ö† <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> No debug information was found for the Python interpreter </span> ‚ö†\n",
       "\n",
       "Without debug information reports showing native traces <span style=\"font-weight: bold\">may not include file names and line numbers</span>. Please use an \n",
       "interpreter built with debug symbols for best results. Check <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://bloomberg.github.io/memray/native_mode.html</span> \n",
       "for more information regarding how memray resolves symbols.\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ö† \u001b[1;33m No debug information was found for the Python interpreter \u001b[0m ‚ö†\n",
       "\n",
       "Without debug information reports showing native traces \u001b[1mmay not include file names and line numbers\u001b[0m. Please use an \n",
       "interpreter built with debug symbols for best results. Check \u001b[4;94mhttps://bloomberg.github.io/memray/native_mode.html\u001b[0m \n",
       "for more information regarding how memray resolves symbols.\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6838e308f904ccea381d136b9b94896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Results saved to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">/home/willsa/git/bedmap-dev/nbs/daft-try/memray-results/tmp92gkp7jy/flamegraph.html</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Results saved to \u001b[1;36m/home/willsa/git/bedmap-dev/nbs/daft-try/memray-results/tmp92gkp7jy/\u001b[0m\u001b[1;36mflamegraph.html\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600\"\n",
       "            src=\"/home/willsa/git/bedmap-dev/nbs/daft-try/memray-results/tmp92gkp7jy/flamegraph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc90e523ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%memray_flamegraph --native --follow-fork --temporal\n",
    "\n",
    "USE_DAFT = True\n",
    "ds_type = \"daft_to_torch\" if USE_DAFT else \"plain_torch\"\n",
    "\n",
    "with TemporaryDirectory() as tmp:\n",
    "    logger.info(\"Downloading test images.\")\n",
    "    dl_hf_images(dir=Path(tmp), max_images=NUM_TEST_IMAGES)\n",
    "    imglob = tmp+\"/*.png\"\n",
    "    logger.info(\"Starting embedding pipeline.\")\n",
    "    embeddings = compute_embeddings(model_name=MODEL_NAME,\n",
    "                                    images_glob = imglob,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    dataset_type=ds_type)\n",
    "    logger.info(\"Done with embedding pipeline.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
