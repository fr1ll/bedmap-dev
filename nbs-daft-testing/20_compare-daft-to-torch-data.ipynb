{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import shutil\n",
    "from glob import glob\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "\n",
    "import memray\n",
    "import daft\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext memray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DAFT: bool = True # else use torch dataset\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MODEL_NAME = \"vit_small_patch14_reg4_dinov2.lvd142m\"\n",
    "IMAGE_GLOB = None\n",
    "IMAGES_FOLDER = \"./tmp-test-images\"\n",
    "\n",
    "TEST_DATASET = \"kvriza8/microscopy_images\"\n",
    "NUM_TEST_IMAGES = 1_000\n",
    "\n",
    "nice_models = [\n",
    "\"mobilenetv3_large_100\",\n",
    "\"vit_small_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "\"vit_large_patch14_reg4_dinov2.lvd142m\",\n",
    "\"aimv2_large_patch14_224.apple_pt_dist\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_hf_images(dataset_name: str = \"kvriza8/microscopy_images\",\n",
    "                 dir: Path = None,\n",
    "                 max_images: int = 50,\n",
    "                 overwrite: bool = True) -> None:\n",
    "\n",
    "    dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "    if overwrite:\n",
    "        shutil.rmtree(dir, ignore_errors=True)\n",
    "        dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i, img_row in enumerate(tqdm(iter(dataset), total=max_images)):\n",
    "        if i >= max_images:\n",
    "            break\n",
    "        img = img_row[\"image\"]\n",
    "        img.save(dir / f\"{i}.png\")\n",
    "\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:22<00:00, 44.72it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tmp_path = Path(IMAGES_FOLDER)\n",
    "dl_hf_images(dir=tmp_path, max_images=NUM_TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embedder:\n",
    "    \"\"\"instantiate pretrained timm model to generate embeddings\"\"\"\n",
    "    def __init__(self, model_name: str, device: torch.device = None):\n",
    "        self.model_name = model_name\n",
    "        # choose device and dtype\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if self.device.type == \"cuda\":\n",
    "            self.dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "        else:\n",
    "            self.dtype = torch.float32\n",
    "\n",
    "        # Create and prepare the model\n",
    "        self.model = timm.create_model(self.model_name, pretrained=True, num_classes=0)\n",
    "        self.model.to(self.device, memory_format=torch.channels_last)\n",
    "        self.model.eval()\n",
    "        self.model = torch.compile(self.model, dynamic=True, mode=\"reduce-overhead\")\n",
    "\n",
    "        # must resolve config to drop unneeded fields\n",
    "        cfg = timm.data.resolve_data_config(self.model.pretrained_cfg)\n",
    "        self.transform = timm.data.create_transform(**cfg)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def embed(self, batch_imgs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"set up input and embed it\"\"\"\n",
    "        batch_imgs = batch_imgs.to(self.device, non_blocking=True, memory_format=torch.channels_last)\n",
    "        if self.device.type == \"cuda\":\n",
    "            with torch.amp.autocast(\"cuda\", dtype=self.dtype):\n",
    "                return self.model(batch_imgs)\n",
    "        return self.model(batch_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@daft.udf(return_dtype=daft.DataType.python())\n",
    "class TransformImageCol:\n",
    "    \"\"\"run timm embedder on an image column\"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.embedder = Embedder(self.model_name)\n",
    "\n",
    "    def __call__(self, batch_images) -> list:\n",
    "        return [self.embedder.transform(Image.fromarray(im)) for im in batch_images.to_pylist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageListIteratorAsDict(Dataset):\n",
    "    def __init__(self, filelist: list[Path], transform: callable):\n",
    "        self.filelist = filelist\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filelist)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image = Image.open(self.filelist[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            return {\"image_transformed\": self.transform(image)}\n",
    "        # return as dict for easy comparison vs. daft\n",
    "        else:\n",
    "            return {\"image\": [image]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_file_list(source: str | list[str]) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Given a folder path, a glob pattern, or a filelist, return a list of image file paths.\n",
    "    If source is a directory, a glob is run using the provided pattern.\n",
    "    If source is a string containing a wildcard, glob is applied.\n",
    "    Otherwise, if it's a list, it is returned directly.\n",
    "    \"\"\"\n",
    "    if isinstance(source, list):\n",
    "        return [Path(s) for s in source]\n",
    "    elif Path(source).is_dir():\n",
    "        patterns = [\"*.png\", \"*.jpg\", \"*.jpeg\"]\n",
    "        return list(chain.from_iterable([Path(source).glob(p) for p in patterns]))\n",
    "    elif isinstance(source, str) and '*' in source:\n",
    "        return [Path(p) for p in glob(source)]\n",
    "    else:\n",
    "        return [source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table class=\"dataframe\">\n",
       "<thead><tr><th style=\"text-wrap: nowrap; max-width:192px; overflow:auto; text-align:left\">path_full_img<br />Utf8</th><th style=\"text-wrap: nowrap; max-width:192px; overflow:auto; text-align:left\">size<br />Int64</th><th style=\"text-wrap: nowrap; max-width:192px; overflow:auto; text-align:left\">image_transformed<br />Python</th></tr></thead>\n",
       "<tbody>\n",
       "<tr><td><div style=\"text-align:left; max-width:192px; max-height:64px; overflow:auto\">file://tmp-test-images/746.png</div></td><td><div style=\"text-align:left; max-width:192px; max-height:64px; overflow:auto\">270443</div></td><td><div style=\"text-align:left; max-width:192px; max-height:64px; overflow:auto\">tensor([[[-0.2342, -0.2171, -0.3712,  ...,  0.5364,  0.4851,  0.3481],<br />         [-0.4397, -0.4397, -0.5596,  ...,  0.5193,  0.3823,  0.1768],<br />         [-0.7308, -0.7650, -0.8849,  ...,  0.4337,  0.2111, -0.1143],<br />         ...,<br />         [ 0.6563,  0.8104,  0.9988,  ...,  2.2489,  2.2489,  2.2318],<br />         [ 0.1083,  0.3138,  0.5364,  ...,  2.1975,  2.1975,  2.1804],<br />         [-0.0287,  0.1768,  0.4166,  ...,  2.1804,  2.1804,  2.1633]],<br /><br />        [[-0.1099, -0.0924, -0.2500,  ...,  0.6779,  0.6254,  0.4853],<br />         [-0.3200, -0.3200, -0.4426,  ...,  0.6604,  0.5203,  0.3102],<br />         [-0.6176, -0.6527, -0.7752,  ...,  0.5728,  0.3452,  0.0126],<br />         ...,<br />         [ 0.8004,  0.9580,  1.1506,  ...,  2.4286,  2.4286,  2.4111],<br />         [ 0.2402,  0.4503,  0.6779,  ...,  2.3761,  2.3761,  2.3585],<br />         [ 0.1001,  0.3102,  0.5553,  ...,  2.3585,  2.3585,  2.3410]],<br /><br />        [[ 0.1128,  0.1302, -0.0267,  ...,  0.8971,  0.8448,  0.7054],<br />         [-0.0964, -0.0964, -0.2184,  ...,  0.8797,  0.7402,  0.5311],<br />         [-0.3927, -0.4275, -0.5495,  ...,  0.7925,  0.5659,  0.2348],<br />         ...,<br />         [ 1.0191,  1.1759,  1.3677,  ...,  2.6400,  2.6400,  2.6226],<br />         [ 0.4614,  0.6705,  0.8971,  ...,  2.5877,  2.5877,  2.5703],<br />         [ 0.3219,  0.5311,  0.7751,  ...,  2.5703,  2.5703,  2.5529]]])</div></td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<small>(Showing first 1 rows)</small>\n",
       "</div>"
      ],
      "text/plain": [
       "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚ path_full_img                  â”† size   â”† image_transformed              â”‚\n",
       "â”‚ ---                            â”† ---    â”† ---                            â”‚\n",
       "â”‚ Utf8                           â”† Int64  â”† Python                         â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ file://tmp-test-images/746.pnâ€¦ â”† 270443 â”† tensor([[[-0.2342, -0.2171, -â€¦ â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
       "\n",
       "(Showing first 1 rows)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imglob = tmp_path.as_posix() +\"/*.png\"\n",
    "images_df = daft.from_glob_path(imglob).with_column_renamed(\"path\", \"path_full_img\")\n",
    "images_df = images_df.with_column(\"image\", daft.col(\"path_full_img\"\n",
    "                                 ).url.download().image.decode(\n",
    "                                     mode=\"RGB\", on_error=\"null\")\n",
    "                                 )\n",
    "images_df = images_df.where(images_df[\"image\"].not_null())\n",
    "\n",
    "TransformImForModel = TransformImageCol.with_init_args(model_name=MODEL_NAME)\n",
    "\n",
    "images_df = images_df.with_column(\"image_transformed\", TransformImForModel(daft.col(\"image\"))\n",
    "                                  ).exclude(\"image\", \"num_rows\")\n",
    "\n",
    "images_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def compute_embeddings(model_name: str,\n",
    "                       batch_size: int = BATCH_SIZE,\n",
    "                       images_df: None | daft.DataFrame = None,\n",
    "                       images_folder: None | Path = None) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Given a model name and a filelist (list of image paths), this function computes and returns a list\n",
    "    of embeddings (one per image). The function instantiates an Embedder, builds a dataset and dataloader,\n",
    "    and processes images in batches.\n",
    "    \"\"\"\n",
    "    embedder = Embedder(model_name=model_name)\n",
    "\n",
    "    if images_df is not None:\n",
    "        dataset = images_df.to_torch_iter_dataset()\n",
    "    elif images_folder:\n",
    "        # use vanilla torch dataloader\n",
    "        image_list = get_file_list(images_folder)\n",
    "        dataset = ImageListIteratorAsDict(image_list, embedder.transform)\n",
    "\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    embeddings = []\n",
    "    for i, batch_images in enumerate(tqdm(dataloader, unit_scale=BATCH_SIZE)):\n",
    "        emb = embedder.embed(batch_images[\"image_transformed\"]).cpu().numpy()\n",
    "\n",
    "        if i == 0:\n",
    "            print(f\"Shape of embedding for one batch: {emb.shape}\")\n",
    "        embeddings.append(emb)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memray WARNING: Correcting symbol for malloc from 0x420620 to 0x7f7477933c60\n",
      "Memray WARNING: Correcting symbol for free from 0x420ab0 to 0x7f7477934370\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c3f750d5a0406fa584ff5528c6482e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ—¡ï¸ ğŸŸ Project: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf24a70129e4fc0901eedd0601e25ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ—¡ï¸ ğŸŸ Filter: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f80bb7e614464491a36c764587e11e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ—¡ï¸ ğŸŸ Project: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%memray_flamegraph --trace-python-allocators --native --leaks --temporal --max-memory-records 10\n",
    "\n",
    "if USE_DAFT:\n",
    "    embeddings = compute_embeddings(MODEL_NAME, BATCH_SIZE, images_df=images_df)\n",
    "else:\n",
    "    # use vanilla torch dataset\n",
    "    embeddings = compute_embeddings(MODEL_NAME, BATCH_SIZE, images_folder=IMAGES_FOLDER)\n",
    "\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
