{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import shutil\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from humanize import naturalsize\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "from transformers import pipeline\n",
    "from itertools import batched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# easy timestamps\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set variables for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MODEL_NAME = \"timm/vit_small_patch14_reg4_dinov2.lvd142m\"\n",
    "TEST_DATASET = \"kvriza8/microscopy_images\"\n",
    "NUM_TEST_IMAGES = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define way to download small test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_hf_images(dataset_name: str = \"kvriza8/microscopy_images\",\n",
    "                 dir: Path = None,\n",
    "                 max_images: int = 64,\n",
    "                 overwrite: bool = True,\n",
    "                 format: str = \"png\") -> None:\n",
    "\n",
    "    dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "    if overwrite:\n",
    "        shutil.rmtree(dir, ignore_errors=True)\n",
    "        dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    image_paths = []\n",
    "    for i, img_row in enumerate(tqdm(iter(dataset), total=max_images)):\n",
    "        if i >= max_images:\n",
    "            break\n",
    "        img = img_row[\"image\"]\n",
    "        image_paths += [(dir / f\"{i}.{format}\")]\n",
    "        img.save(image_paths[-1])\n",
    "\n",
    "    print(f\"Size of images on disk: {naturalsize(sum([p.stat().st_size for p in image_paths]))}\")\n",
    "\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-29 13:21:02.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading test images.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:07<00:00, 33.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of images on disk: 19.5 MB\n",
      "\u001b[32m2025-03-29 13:21:11.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mInstantiating pipeline.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-29 13:21:12.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mStarting embedding pipeline.\u001b[0m\n",
      "\u001b[32m2025-03-29 13:23:13.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mDone with embedding pipeline.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c5bc552db94774b3ac431d5c03ab05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">⚠ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> No debug information was found for the Python interpreter </span> ⚠\n",
       "\n",
       "Without debug information reports showing native traces <span style=\"font-weight: bold\">may not include file names and line numbers</span>. Please use an \n",
       "interpreter built with debug symbols for best results. Check <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://bloomberg.github.io/memray/native_mode.html</span> \n",
       "for more information regarding how memray resolves symbols.\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "⚠ \u001b[1;33m No debug information was found for the Python interpreter \u001b[0m ⚠\n",
       "\n",
       "Without debug information reports showing native traces \u001b[1mmay not include file names and line numbers\u001b[0m. Please use an \n",
       "interpreter built with debug symbols for best results. Check \u001b[4;94mhttps://bloomberg.github.io/memray/native_mode.html\u001b[0m \n",
       "for more information regarding how memray resolves symbols.\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83c242a80d54d54a77ba5a653389bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Results saved to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">/home/willsa/git/bedmap-dev/nbs/daft-try/memray-results/tmpcw2epyw8/flamegraph.html</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Results saved to \u001b[1;36m/home/willsa/git/bedmap-dev/nbs/daft-try/memray-results/tmpcw2epyw8/\u001b[0m\u001b[1;36mflamegraph.html\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600\"\n",
       "            src=\"/home/willsa/git/bedmap-dev/nbs/daft-try/memray-results/tmpcw2epyw8/flamegraph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f349e887920>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%memray_flamegraph --native --follow-fork --temporal\n",
    "\n",
    "with TemporaryDirectory() as tmp:    \n",
    "    logger.info(\"Downloading test images.\")\n",
    "    dl_hf_images(dir=Path(tmp), max_images=NUM_TEST_IMAGES)\n",
    "    imglob = tmp+\"/*.png\"\n",
    "    imagepaths = list(Path(tmp).glob(\"*.png\"))\n",
    "    embeddings = []\n",
    "    batch = []\n",
    "\n",
    "    logger.info(\"Instantiating pipeline.\")\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    pipe = pipeline(task=\"image-feature-extraction\",\n",
    "                    model=MODEL_NAME, device=DEVICE, pool=True, use_fast=True)\n",
    "            \n",
    "\n",
    "    logger.info(\"Starting embedding pipeline.\")\n",
    "\n",
    "    for batch_paths in batched(imagepaths, BATCH_SIZE):\n",
    "        batch = [Image.open(p.as_posix()).convert(\"RGB\") for p in batch_paths]\n",
    "        embeddings += pipe(batch)\n",
    "\n",
    "    logger.info(\"Done with embedding pipeline.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
