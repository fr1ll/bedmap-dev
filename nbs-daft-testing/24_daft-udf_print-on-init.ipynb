{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import shutil\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import daft\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from humanize import naturalsize\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# easy timestamps\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swap daft's download->decode for pillow's open\n",
    "\n",
    "Adapt [test 21 notebook (torch vs. daft-to-torch)](https://github.com/fr1ll/bedmap-dev/blob/clearer-atlas/nbs/daft-try/21_compare-daft-to-torch-data_clean.ipynb):\n",
    "- add concurrency level parameter for parity with nb 22, but keep as None (default)\n",
    "- Use Pillow to do image open in daft as suggested by Colin Ho in slack discussion\n",
    "\n",
    "Result: Peak memory usage drops from 3.3 Gb to 3.0 Gb. This is still more than plain torch at 1.9 Gb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set variables for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DAFT: bool = True # else use torch dataset\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MODEL_NAME = \"vit_small_patch14_reg4_dinov2.lvd142m\"\n",
    "TEST_DATASET = \"kvriza8/microscopy_images\"\n",
    "NUM_TEST_IMAGES = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define way to download small test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_hf_images(dataset_name: str = \"kvriza8/microscopy_images\",\n",
    "                 dir: Path = None,\n",
    "                 max_images: int = 64,\n",
    "                 overwrite: bool = True,\n",
    "                 format: str = \"png\") -> None:\n",
    "\n",
    "    dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "    if overwrite:\n",
    "        shutil.rmtree(dir, ignore_errors=True)\n",
    "        dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    image_paths = []\n",
    "    for i, img_row in enumerate(tqdm(iter(dataset), total=max_images)):\n",
    "        if i >= max_images:\n",
    "            break\n",
    "        img = img_row[\"image\"]\n",
    "        image_paths += [(dir / f\"{i}.{format}\")]\n",
    "        img.save(image_paths[-1])\n",
    "\n",
    "    print(f\"Size of images on disk: {naturalsize(sum([p.stat().st_size for p in image_paths]))}\")\n",
    "\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define timm-based embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embedder:\n",
    "    \"\"\"instantiate pretrained timm model to generate embeddings\"\"\"\n",
    "    def __init__(self, model_name: str, device: torch.device = None):\n",
    "        self.model_name = model_name\n",
    "        # choose device and dtype\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if self.device.type == \"cuda\":\n",
    "            self.dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "        else:\n",
    "            self.dtype = torch.float32\n",
    "\n",
    "        # Create and prepare the model\n",
    "        self.model = timm.create_model(self.model_name, pretrained=True, num_classes=0)\n",
    "        self.model.to(self.device, memory_format=torch.channels_last)\n",
    "        self.model.eval()\n",
    "        self.model = torch.compile(self.model, dynamic=True, mode=\"reduce-overhead\")\n",
    "\n",
    "        # must resolve config to drop unneeded fields\n",
    "        cfg = timm.data.resolve_data_config(self.model.pretrained_cfg)\n",
    "        self.transform = timm.data.create_transform(**cfg)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def embed(self, batch_imgs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"set up input and embed it\"\"\"\n",
    "        batch_imgs = batch_imgs.to(self.device, non_blocking=True, memory_format=torch.channels_last)\n",
    "        if self.device.type == \"cuda\":\n",
    "            with torch.amp.autocast(\"cuda\", dtype=self.dtype):\n",
    "                return self.model(batch_imgs)\n",
    "        return self.model(batch_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define two types of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@daft.udf(return_dtype=daft.DataType.python(), batch_size=BATCH_SIZE)\n",
    "class LoadPillowTransformDaft:\n",
    "    \"\"\"run timm embedder on an image column\"\"\"\n",
    "    def __init__(self, transform: callable, ):\n",
    "        self.transform = transform\n",
    "        self._pil_load = lambda x: Image.open(x).convert(\"RGB\")\n",
    "        print(\"Initializing daft UDF\")\n",
    "\n",
    "    def __call__(self, batch_paths: daft.Series) -> list:\n",
    "        batch_paths = [p.replace(\"file://\",\"\") for p in batch_paths.to_pylist()]\n",
    "        return [self.transform(self._pil_load(p)) for p in batch_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TorchImageIterAsDict(Dataset):\n",
    "    def __init__(self, filelist: list[Path], transform: callable):\n",
    "        self.filelist = filelist\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filelist)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image = Image.open(self.filelist[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            return {\"image_transformed\": self.transform(image)}\n",
    "        # return as dict for easy comparison vs. daft\n",
    "        else:\n",
    "            return {\"image\": [image]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daft_to_torch_iter_from_glob(image_glob: str, transform: callable,\n",
    "                                 num_concurr: int | None):\n",
    "    \"\"\"generate a torch image dataset via daft from a glob\"\"\"\n",
    "\n",
    "    images_df = daft.from_glob_path(image_glob)\n",
    "\n",
    "    TransformFromPilForModel = LoadPillowTransformDaft.with_init_args(transform=transform).with_concurrency(num_concurr)\n",
    "    images_df = images_df.with_column(\"image_transformed\", TransformFromPilForModel(daft.col(\"path\"))\n",
    "                                    ).exclude(\"num_rows\")\n",
    "    return images_df.to_torch_iter_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_iter_from_glob(image_glob: str, transform: callable):\n",
    "    \"\"\"generate a torch image dataset via daft from a glob\"\"\"\n",
    "\n",
    "    image_list = [Path(p) for p in glob(image_glob)]\n",
    "    return TorchImageIterAsDict(image_list, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedding computation pipeline including dataset instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(model_name: str, images_glob: str, batch_size: int = BATCH_SIZE,\n",
    "                       dataset_type: str = \"plain_torch\", daft_nconcurr: int | None = 1\n",
    "                       ) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Return a list of embeddings from a glob of images.\n",
    "    Uses a timm pretrained model to generate embeddings\n",
    "    \"\"\"\n",
    "    logger.info(\"Instantiating embedding model.\")\n",
    "    embedder = Embedder(model_name=model_name)\n",
    "\n",
    "    logger.info(f\"Creating dataset of type {dataset_type}.\")\n",
    "    if dataset_type == \"daft_to_torch\":\n",
    "        dataset = daft_to_torch_iter_from_glob(images_glob, embedder.transform, daft_nconcurr)\n",
    "    elif dataset_type == \"plain_torch\":\n",
    "        dataset = torch_iter_from_glob(images_glob, embedder.transform)\n",
    "    else:\n",
    "        raise ValueError(\"Dataset type must be `daft_to_torch` or `plain_torch`.\")\n",
    "\n",
    "    logger.info(\"Creating dataloader.\")\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    logger.info(\"Generating embeddings.\")\n",
    "    for i, batch_images in enumerate(tqdm(dataloader, unit_scale=BATCH_SIZE)):\n",
    "        emb = embedder.embed(batch_images[\"image_transformed\"]).cpu().numpy()\n",
    "        # if i == 0:\n",
    "        #     print(f\"Shape of embedding for one batch: {emb.shape}\")\n",
    "        embeddings.append(emb)\n",
    "    logger.info(\"Stacking embeddings.\")\n",
    "    embeddings = np.vstack(embeddings)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Do memory profiling with one type of dataset\n",
    "\n",
    "Results written near top of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-16 21:47:42.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloading test images.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memray WARNING: Correcting symbol for malloc from 0x420620 to 0x7ff079126c60\n",
      "Memray WARNING: Correcting symbol for free from 0x420ab0 to 0x7ff079127370\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 256/256 [00:13<00:00, 18.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of images on disk: 19.5 MB\n",
      "\u001b[32m2025-03-16 21:48:00.079\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mStarting embedding pipeline.\u001b[0m\n",
      "\u001b[32m2025-03-16 21:48:00.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompute_embeddings\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mInstantiating embedding model.\u001b[0m\n",
      "\u001b[32m2025-03-16 21:48:02.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompute_embeddings\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mCreating dataset of type daft_to_torch.\u001b[0m\n",
      "\u001b[32m2025-03-16 21:48:02.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompute_embeddings\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mCreating dataloader.\u001b[0m\n",
      "\u001b[32m2025-03-16 21:48:02.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompute_embeddings\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mGenerating embeddings.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing daft UDF\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5cbd6d967c4a7bbca59e642044b77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üó°Ô∏è üêü Project: 00:00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [03:58,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-16 21:52:00.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompute_embeddings\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mStacking embeddings.\u001b[0m\n",
      "\u001b[32m2025-03-16 21:52:01.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mDone with embedding pipeline.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d4183479494c6cb520f3c4b1f8e683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ö† <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> No debug information was found for the Python interpreter </span> ‚ö†\n",
       "\n",
       "Without debug information reports showing native traces <span style=\"font-weight: bold\">may not include file names and line numbers</span>. Please use an \n",
       "interpreter built with debug symbols for best results. Check <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://bloomberg.github.io/memray/native_mode.html</span> \n",
       "for more information regarding how memray resolves symbols.\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ö† \u001b[1;33m No debug information was found for the Python interpreter \u001b[0m ‚ö†\n",
       "\n",
       "Without debug information reports showing native traces \u001b[1mmay not include file names and line numbers\u001b[0m. Please use an \n",
       "interpreter built with debug symbols for best results. Check \u001b[4;94mhttps://bloomberg.github.io/memray/native_mode.html\u001b[0m \n",
       "for more information regarding how memray resolves symbols.\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fee5d3cf3ca46eebf22384957dc13be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%memray_flamegraph --native --follow-fork --temporal\n",
    "\n",
    "USE_DAFT = True\n",
    "CONCURRENCY_LEVEL = None\n",
    "ds_type = \"daft_to_torch\" if USE_DAFT else \"plain_torch\"\n",
    "\n",
    "with TemporaryDirectory() as tmp:\n",
    "    logger.info(\"Downloading test images.\")\n",
    "    dl_hf_images(dir=Path(tmp), max_images=NUM_TEST_IMAGES)\n",
    "    imglob = tmp+\"/*.png\"\n",
    "    logger.info(\"Starting embedding pipeline.\")\n",
    "    embeddings = compute_embeddings(model_name=MODEL_NAME,\n",
    "                                    images_glob = imglob,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    dataset_type=ds_type,\n",
    "                                    daft_nconcurr=CONCURRENCY_LEVEL)\n",
    "    logger.info(\"Done with embedding pipeline.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
